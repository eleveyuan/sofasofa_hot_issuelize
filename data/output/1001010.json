{"id": "1001010", "question": "为什么机器学习中的优化问题很少用到牛顿法？", "description": "<div class=\"col-md-11 col-xs-10\"><p>机器学习中最常用到的就是梯度下降法，为什么很少用到收敛速度更快的牛顿法？</p><p><br/></p></div>", "viewer": 8037, "tags": ["数学", "数值计算", "最优化", "开放问题"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px;\">牛顿法需要计算二阶导数，也就是说要计算一个</span><a href=\"http://sofasofa.io/forum_main_post.php?postid=1000453\"><span style=\"font-size: 14px;\">Hessian矩阵</span></a><span style=\"font-size: 14px;\">。</span></p><ul><li><span style=\"font-size: 14px;\">有些时候，损失函数的二阶导数的显式方程未必那么好求。机器学习不同于传统的优化问题，前者的目标函数可能是自己的定义的，未必是光滑的。</span></li><li><span style=\"font-size: 14px;\">机器学习问题与传统优化的另外一个区别是，前者的维数通常会很大，经常会上百或者上千乃至上万。如果数据集有n个特征，那么这个Hessian矩阵就是n x n的，我们需要求n平方个点，而sgd只需要求n个点。</span></li><li><span style=\"font-size: 14px;\">当n很大的时候，我们需要存一个n x n的矩阵，在空间也会是一个负担。</span></li></ul><p><span style=\"font-size: 14px;\">其次，牛顿法的步长是通过导数计算而来的。所以当临近鞍点的时候，步长会变得越来越小，这样牛顿法就很容易陷入鞍点之中。而sgd的步长是预设的固定值，相对容易跨过一些鞍点。</span></p><p><br/></p></div>", "lvl2_answer": ["谢谢！解释得很清楚！"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>首先我先说说梯度下降。\n</p><p>其实梯度下降法在机器学习中也极小被直接使用。通常使用的也是梯度下降的改进版本，比如随机梯度下降、小批量梯度下降、<a href=\"http://sofasofa.io/forum_main_post.php?postid=1000410\">随机平均梯度下降</a>等等。</p><p><br/></p><p>\n</p><p>\n</p><p>下面说说牛顿法。\n</p><ul><li><span style=\"font-size: 14px;\">牛顿法需要二阶导数，也就是Hessian矩阵。\n</span></li><li><span style=\"font-size: 14px;\">牛顿法需要Hessian矩阵正定，如果是非正定的话，牛顿法会陷在鞍点。\n</span></li><li><span style=\"font-size: 14px;\">最重要的是，牛顿法的迭代中需要对Hessian矩阵求逆，$$x_{i+1} = x_i - H^{-1} \\nabla_x J(x_{i})$$熟悉矩阵计算的朋友一定知道这件事有多昂贵。</span></li></ul><p>\n</p><p><br/></p><p>败也萧何，败也萧何。牛顿法的优势也正是二阶收敛速度。所以就不少拟牛顿法，既保留二阶收敛速度，又尽量避免上面的缺陷。其中比较出名的，在机器学习中经常出现的有Broyden–Fletcher–Goldfarb–Shanno(BFGS)算法，它用低秩分解来代替求逆。后来还有的L-BFGS算法，降低了算法对内存的需求。对于很多多维机器学习问题，这个优势是很重要的。\n<span style=\"font-size: 1rem;\"></span></p><p><b>所以说牛顿法的思想在机器学习的优化问题中依然是很常见的。</b></p><p><b><br/></b></p><p><b>\n</b></p><p>\n</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>我觉得主要是因为<a href=\"http://sofasofa.io/forum_main_post.php?postid=1000966\">牛顿法是二阶</a>的，求二阶导数在大规模计算中应该是个很昂贵（费时间）的事情吧。</p><p><br/></p></div>", "lvl2_answer": ["谢谢！"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>虽然牛顿法不常用，但是各种拟牛顿法还是层出不穷的呀</p><p><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>架不住计算量</p></div>", "lvl2_answer": []}]}