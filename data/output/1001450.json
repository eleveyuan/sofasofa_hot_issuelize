{"id": "1001450", "question": "神经网络中梯度消弥的原因是什么？", "description": "<div class=\"col-md-11 col-xs-10\"><p>神经网络中梯度会逐渐变成零，一般称为梯度消弥</p><p>造成梯度消弥的原因是什么？</p><p><br/></p></div>", "viewer": 4285, "tags": ["统计/机器学习", "数值计算", "深度学习", "人工神经网络"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>一般是比较深的神经网络容易出现梯度消失的现象</p><p>如果激活函数的梯度是有界的，比如sigmoid的导数就是在(0, 1)之间，根据链式法则，梯度会一层层递减，当层数很多时，最终梯度会非常小，这就造成了所谓梯度消失的现象。</p><p><br/></p></div>", "lvl2_answer": []}]}