{"id": "1002696", "question": "如果特征都是非负的，那么用RELU作为激活函数还有意义吗？", "description": "<div class=\"col-md-11 col-xs-10\"><p>在用神经网络训练的时候，如果训练集里的特征的数值都是非负的，那么用RELU作为激活函数还有意义吗？</p><p>是不是和没有激活函数的效果一样的？</p></div>", "viewer": 5452, "tags": ["统计/机器学习", "人工神经网络"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px;\">激活函数的输入是$x^Tw$，$w$是网络参数，可正可负；$x$是输入，全为正。明显$x^Tw$的范围是实数，ReLU肯定有用。</span></p><p><span style=\"font-size: 14px;\"><br/></span></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>当然不是，relu的输入时特征和参数相乘，参数矩阵一般给到随机值作为初始，所以输入是正是负不可知，但肯定有正有负；就算特征全正，参数一开始给全1矩阵，也会在梯度下降过程调参也有可能把参数调参成负值来拟合模型；只有在非常特殊的情况下，relu表现和无激活函数一样；这种情况也许都不一定要使用高级神经网络<br/><br/></p></div>", "lvl2_answer": []}]}