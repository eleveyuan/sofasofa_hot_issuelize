{"id": "1001229", "question": "关于小批量K均值（mini-batch K Means）的问题", "description": "<div class=\"col-md-11 col-xs-10\"><p>最近看到一个叫做小批量K均值（mini-batch K Means）的聚类方法。</p><p>K均值我懂，SGD里的小批量我也懂。</p><p>【不懂就问】</p><p>但是这两个合在一起是什么意思？</p><p>小批量K均值和正常的K均值什么关系？这里的小批量又是什么意思？</p><p><br/></p></div>", "viewer": 7649, "tags": ["统计/机器学习", "无监督学习"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>mini-batch K Means是对普通的K Means计算效率的优化。</p><p>在普通的K Means的计算过程中，每次更新各聚类中心点时，需要计算所有点和每个聚类中心点的距离，所以代价特别昂贵。</p><p>而在mini-batch K Means的计算过程中，每次更新各聚类中心点时，先从所有数据中随机地选取一个小集合（也就是这里的mini-batch），根据这个集合中的数据点，来更新各聚类的中心点。下一次更新时，再重新从所有数据点中选取一个随机的小集合，如此重复，直到达到收敛条件。</p><p><br/></p><p>mini-batch的思想就是用部分数据，而不是全部数据，来更新模型的参数。所以从这一点来说，mini-batch K means和mini-batch sgd是同一个思想。</p><p><br/></p></div>", "lvl2_answer": ["讲得挺清楚的。"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>为了加快k means算法的计算速度，每次迭代的时候，只选了小批量的随机数据，而不是全部数据，这个思想就是小批量k means。</p><p>更多细节可以参考<a href=\"http://scikit-learn.org/stable/modules/clustering.html#mini-batch-kmeans\" target=\"_blank\"> Mini Batch K-Means in Sklearn</a></p><p><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>一个类似的问题：<a href=\"http://sofasofa.io/forum_main_post.php?postid=1003172\" target=\"_blank\">Mini-batch K-Means实现online learning的原理是什么？</a></p><p>答案说得很清楚</p></div>", "lvl2_answer": []}]}