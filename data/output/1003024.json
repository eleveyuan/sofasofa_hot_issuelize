{"id": "1003024", "question": "如果用xgboost模型，那么还需要先做feature selection吗？", "description": "<div class=\"col-md-11 col-xs-10\"><p>如果用xgboost模型，那么还需要先做feature selection吗？</p><p>还是说不管三七二十一直接把完整的数据一起扔到xgboost里训练？</p></div>", "viewer": 7219, "tags": ["统计/机器学习", "监督式学习", "特征选择"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>当然是需要的。</p><p>逆向思维一下，如果你加一堆不相关的变量到你的数据集里，然后再训练一次模型，你觉得模型的精度是下降的可能性大还是提高的可能性大？</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>一般来说，xgboost（包括random forests）对冗余的变量是不敏感的。</p><p>但是正如MangoCoke说的，“垃圾”变量肯定是对模型有负面影响的。</p><p>另外一方面，xgboost或者说每棵树在选择分叉点的时候，都是以贪婪的方式选择局部最优，所以有些特征可能在局部不错，但是从整体上看降低了模型整体的精度。</p><p>再有就是不相关或者重复的变量会影响最终的feature importance。</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>多余的变量对xgboost或者gbdt都是有影响的，所以做特征选择肯定是有帮助的。</p><p>另外可以参考一下：<a href=\"http://sofasofa.io/forum_main_post.php?postid=1001161\" target=\"_blank\">对于xgboost，还有必要做很多特征工程吗？</a></p></div>", "lvl2_answer": []}]}