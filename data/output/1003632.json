{"id": "1003632", "question": "怎么理解platt scaling？", "description": "<div class=\"col-md-11 col-xs-10\"><p>大家好，我对platt scaling不是很理解，大概知道是SVM里用得到概率输出的方法。</p><p>具体应该怎么理解platt scaling？</p></div>", "viewer": 6800, "tags": ["统计/机器学习", "监督式学习"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><b>platt's scaling本质上就是利用一个逻辑回归将SVM的输出值映射为概率。</b></p><p>因为SVM的输出值是样本和决策边界的距离而非概率，输出值大于0是正样本，输出值小于0是负样本。</p><p>platt's scaling就利用这个输出值作为样本的特征，再利用样本的标签，训练一个一维数据的逻辑回归。这个逻辑回归的最后输出的概率值就是platt's scaling后的预测概率。</p><p>假设第$i$个样本的特征为$X_i$，SVM的决策函数的输出结果为$f(X_i)$，那么platt's scaling之后预测这个样本为正的概率为</p><p>$$P(1|X_i)=\\frac{1}{1+e^{af(X_i) + b}}$$</p><p>其中$a$和$b$是通过训练集训练得到的。</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>从神经网络的角度来说，platt scaling相当于在最外面加一个sigmoid输出层</p></div>", "lvl2_answer": ["这个解释很直白"]}]}