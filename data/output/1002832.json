{"id": "1002832", "question": "关于神经网络的性能", "description": "<div class=\"col-md-11 col-xs-10\"><p>最近再看周志华的《机器学习》，其中神经网络一章提到，原文我记不清了，大意是说：数学上已经证明：只要你深度足够深，参数挑的足够合适，一个神经网络是可以拟合出任意可能的判决形式的；</p><p><br/></p><p>本人菜鸡，深度学习做得比较少，想问问深度学习大神们怎么看这句话。</p></div>", "viewer": 3198, "tags": ["统计/机器学习", "深度学习", "人工神经网络"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>你说的应该是<a href=\"http://sofasofa.io/forum_main_post.php?postid=1002456\" target=\"_blank\">Universal approximation theorem</a>的引申。</p><p>Universal approximation theorem是说对于一个前馈神经网络，哪怕只有一个隐藏层，它也能无限逼近任何有界连续函数。</p></div>", "lvl2_answer": ["谢谢老哥"]}]}