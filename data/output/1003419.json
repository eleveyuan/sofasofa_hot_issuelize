{"id": "1003419", "question": "pyspark groupby 加权平均？", "description": "<div class=\"col-md-11 col-xs-10\"><p>比如说对df['id']进行groupby，df['value']是数值，df['weight']是权重，怎么对pyspark dataframe对id进行加权平均，有没有类似这样的功能</p><pre><code class=\"python\">df.groupby('id').agg(weight_avg('value', 'weight'))</code></pre><p>新上手pyspark，不大熟悉</p></div>", "viewer": 5140, "tags": ["统计/机器学习", "数据预处理", "Python"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>并没有直接做加权平均的函数，你可以自己写出表达式来</p><pre><code class=\"python\">from pyspark.sql import functions as F\n\ndf = df.groupby('id')\\\n        .agg(F.sum(F.col('value') * F.col('weight'))/F.sum(F.col('weight'))\\\n        .alias('w_mean'))</code></pre><p>你也可以自己按照上面的式子写udf</p></div>", "lvl2_answer": []}]}