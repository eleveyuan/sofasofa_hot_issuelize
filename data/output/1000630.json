{"id": "1000630", "question": "随机森林如何调参？", "description": "<div class=\"col-md-11 col-xs-10\"><p>我知道是通过交叉验证来比较不同参数下模型的好坏，但是有没有大致的调参的方向？哪些参数尤其重要？</p></div>", "viewer": 12466, "tags": ["统计/机器学习", "监督式学习", "模型验证"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>树的数量m：m越大越好，但是太大会影响训练速度。</p><p><br/></p><p>树的最大深度d：d越大，越容易过拟合；越小，越容易欠拟合。如果d没有上限，那么每棵树都会是一棵完整的决策树，设置了d，相当于给树进行了<a href=\"http://sofasofa.io/forum_main_post.php?postid=1000336\">剪枝</a>。</p><p><br/></p><p>特征数量f：假设数据集一共有p个特征，树的每次分叉会随机使用f个特征，f一般设定为sqrt(p)，但是也可以设定为任意数值。f越大，每棵树越像，为了增加多样性，f一般不会接近p。除了设置为固定值外，也通常可以设定为0.3*p，0.5*p等。</p><p><br/></p><p>叶节点最小样本数量s：当某个节点上的样本数量小于s时，树就停止生长，s越小，单个决策树越容易过拟合。设置一个较大的s，也有剪枝的作用，s=5，10，50，都很常见。</p><p><br/></p><p><br/></p><p>不同的数据集可能有不同的最佳参数，有的最佳参数也许也很反常。但是归根结底，利用交叉验证来调整参数，选择最佳的参数组合才是最可靠的随机森林调参方法。</p><p><br/></p><p>----更新----</p><p>上文根据Zealing的指正，已经修改。</p><p><br/></p></div>", "lvl2_answer": ["谢谢痴汉！", "你回答中关于特征数量f，有问题。应该是每个节点分裂时，随机选出f个特征，然后再其中一个特征上找出最好的分裂。换句话，每棵树能见到所有的特征，而每个节点只能看见随机的f个特征。", "感谢，看来我一直对RF的这点也存在误解", "谢谢Zealing的指正，我去看了下，你说的对的。我修改下答案。"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>对@数据痴汉 提到的这几个参数进行grid search，然后对比cross validation之后的结果。</p><p><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>也可以参考<a href=\"http://sofasofa.io/forum_main_post.php?postid=1000868\">xgb调参的方法</a></p><p><br/></p></div>", "lvl2_answer": []}]}