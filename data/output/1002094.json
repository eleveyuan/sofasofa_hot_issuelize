{"id": "1002094", "question": "线性回归有精确的解析解为什么还要用梯度下降得到数值解？", "description": "<div class=\"col-md-11 col-xs-10\"><p>学完了SGD才突然想起来，线性回归明明是有解析解的，而且是精确的。</p><p>既然有精确的解析解，为什么还要用梯度下降得到数值解呢？</p><p><br/></p></div>", "viewer": 7423, "tags": ["统计/机器学习", "数值计算", "最优化", "回归分析"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>线性回归$y=Xw+\\epsilon$其中$X$是输入，$y$是输出，$w$是未知参数,$\\epsilon$是噪声。所谓的解析解就是最小二乘法($w=argmin_w|Xw-y|^2$)意义下的对$X$的pseudoinverse $w=(X^TX)^{-1}X^Ty$.这有个前提是$X$和$y$服从联合正态分布。如果$X$和$y$不是联合正态分布，那么这个解析解就不是精确的。实际中X和y的分布有可能不知道或者太复杂，不能化简一步得到解析解，所以还是要靠SGD之类的循环算法去逼近最优解。</p><p>加一个blog <a href=\"https://wiseodd.github.io/techblog/2017/01/05/bayesian-regression/\">https://wiseodd.github.io/techblog/2017/01/05/bayesian-regression/</a></p><p>还可以去看Murphy, Kevin P. Machine learning: a probabilistic perspective.</p><p>--------------------</p><p>1.如果X中含有非独立的变量，$rank(\\Sigma_{XX})=rank(X^TX)&lt;d$,d是X维度，则$X^TX$不可逆。此时最小二乘无解。</p><p>2.如果X中的变量非常相关，$X^TX$的condition number很大，$(X^TX)^{-1}$中误差会被放大，而且会非常大。GD不用求$X^TX$的逆，所以结果比较可信。</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>我觉得主要有两个原因。</p><p><b>第一个原因：速度快</b>。这个当然是最主要的原因之一了。Zealing的回答里已经提到了，精确解是需要求矩阵乘积和矩阵逆的，这个计算量是比较恐怖的。SGD比这个快多了。</p><p><b>第二个原因：SGD是线上算法（online）。</b>有新数据进入的时候，不需要重新计算，而精确的矩阵求解是做不到线上算法的。</p><p><br/></p></div>", "lvl2_answer": ["我觉得online是个很大的优势"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>之前好像在某本书里看到“当训练集特征维度数较大或者样本数过多时，使用GD应该可以提高训练的速度”，不知是不是这样。</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>复杂模型拟合的高阶连续函数应该都是非线性的吧</p></div>", "lvl2_answer": []}]}