{"id": "1005997", "question": "lightgbm怎么处理分类变量？", "description": "<div class=\"col-md-11 col-xs-10\"><p>LGBMClassifier在fit时可以输入categorical_feature，这样所有的分类变量不需要进行one-hot encoding。</p><p>那么lgbm到底是怎么处理分类变量的？底层是什么原理？</p></div>", "viewer": 6982, "tags": ["统计/机器学习", "监督式学习", "数据预处理"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>用独热编码来表示分类特征是很常见的，但是这种方法对于树模型来说不是最优的。特别是对于高基数分类特征，在单一热门特征上构建的树往往是不平衡的，需要增长得非常深才能达到良好的准确性。</p><p>最佳解决方案是通过将类别分类为2个子集来对类别特征进行拆分，而不是使用独热编码。 如果特征具有k个类别，则存在2 ^（k-1）-1个可能的分区。 但是对于回归树有一个有效的解决方案[1]。 它需要大约O（k * log（k））来找到最佳分区。\n</p><p>基本思想是在每次分组时根据训练目标对类别进行分类。 更具体地说，LightGBM根据其累积值（sum_gradient / sum_hessian）对直方图（用于分类特征）进行排序，然后在排序的直方图上找到最佳分割。</p><p><br/></p><p>总结一下，就是加快了训练速度。</p><p><br/></p><p>[1] Walter D. Fisher. “On Grouping for Maximum Homogeneity.” Journal of the American Statistical Association. Vol. 53, No. 284 (Dec., 1958), pp. 789-798.</p><p><a href=\"https://lightgbm.readthedocs.io/en/latest/Features.html#optimal-split-for-categorical-features\" target=\"_blank\">参考自官方文档</a><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>它的原理是把这个categorical feature下所有的level分成两个子集，如果是3个level，就有3种分法，k个level就有$2^{k-1}-1$个分法，从中挑选最佳的一种分法。</p><p>在LGBMClassifier或者LGBMRegressor中，设置categorical_feature时一定要先进行label encoding，也就是分类特征的取值必须是整数。</p></div>", "lvl2_answer": []}]}