{"id": "1002384", "question": "GBDT+LR的工作原理？", "description": "<div class=\"col-md-11 col-xs-10\"><p>很多地方看到GDBT+LR的模型，据说很厉害。</p><p>工作原理到底是什么？是简单的将GBDT和LR进行stack吗？</p></div>", "viewer": 8494, "tags": ["统计/机器学习", "回归分析", "监督式学习"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>简单地说，就是把gbdt的输出，作为logistic regression的输入，最后得到一个logistic regression模型。</p><p>例如，gbdt里有$3$棵树$T_1,T_2,T_3$，每棵树的叶节点个数为$4$，第$i$个树的第$j$个叶节点是$L_{i,j}$。</p><p>当gdbt训练完成之后，样本$X_1$在第一棵树中被分到了第$3$个叶节点上，也就是$L_{1,3}$，那么这个样本在$T_1$上的向量表达为$(0,0,1,0)$。</p><p>样本$X_1$在$T_2$被分到了$L_{2,1}$，那么$X_1$在$T_2$上的向量表达为$(1,0,0,0)$</p><p>样本$X_1$在$T_3$被分到了$L_{3,4}$，那么$X_1$在$T_3$上的向量表达为$(0,0,0,1)$</p><p>那么$X_1$在整个gbdt上的向量表达为</p><p>$$(0,0,1,0,1,0,0,0,0,0,0,1)$$</p><p>所以每个样本都会被表示为一个长度为12的0-1向量，其中有3个数值是1。</p><p>然后这类向量就是LR模型的输入数据。</p><p><br/></p></div>", "lvl2_answer": ["简洁明了", "赞", "谢谢分享"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>简单来说是将GBDT中每棵树的叶节点作为一个新的特征（01变量，如果某样本落在该叶节点上就是1，否则为0）。可以在r中用xgboost的create.features函数手动玩一下。</p></div>", "lvl2_answer": ["是不是可以认为类似深度学习里的transfer learning"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>这个是现在广告业界的套路，好像叫什么ctr。</p><p>因为xgb等的梯度提升树训练过于复杂，而lr众所周知过于简单，只能发现线性过于简单，对于交互项和非线性关系没有辨识度，于是大佬们就开发一个新套路，用gbdt的训练数据，gbdt=梯度+树，基于树模型，就有了交叉和非线性，然后把叶子节点放到lr里，这不解决了lr的缺点。</p><p>当然，我并没有做过，所以就这样简单粗略说一下，等待大佬给出学术性专业回答。</p><p>ps：至于厉害不厉害，就不知道了，工业界比较厉害吧，毕竟谷歌阿里都在用，至于比赛的话，我就不晓得了，我还指望哪个大佬能指点一下我，最近瓶颈啊，完全不知道怎么清洗数据，每次搞完特征，发现还不如原特征往里面塞</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px;\">一棵树的叶节点把数据空间非线性的划分为 $2^d$个互不相交的子空间，$d$为树的深度。多棵树就有多种划分。这里把多棵树子空间的序号的one-hot编码组合作为输入数据$x$。$x_i=1$，表示数据点在第i个子空间。每一个子空间都是一个cluste。这个技术的关键思想是用是否在子空间的指示标签（binary indicator）代表子空间中的所有点，类似于kmeans中mean代表所有点的值。最后通过所有子空间的影响力的和算出$P(\\hat{y}=1)$。</span></p><p>再看LR，$P(y=1)=\\sigma(w^Tx)=\\dfrac{1}{1+e^{-w^Tx}}$，如果logit $w^Tx$值越大，预测标签$y=1$的概率越大；如果$w^Tx$越小（$w^Tx&lt;0$），$y=0$的概率越大。在训练时，如果第i个子空间中$y=1$的数据点很多，相应的$w_i$会被训练成一个较大的正数。在预测时，如果$x_i=1$,则$w_ix_i=w_i$会贡献一个较大的logit，驱使$\\hat{y}=1$的概率变大。如果$x_i=0$,$w_ix_i=0$对预测没影响。$w_i$可以看做第i个子空间对$P(y=1)$的影响力。</p><p><span style=\"font-size: 14px;\">打个比方，小明上的小学，初中，高中，大学都是好学校，最后小明学习成绩好的概率就会高。其中小学的划分就是一棵树，初中是另外一棵树。新造的$x$就是所读学校的编号的组合。$w_i$就是第i个学校对学习成绩好的影响。</span></p><p><span style=\"font-size: 14px;\">计算的链条是原始数据$x_0$-(GBDT)-&gt; 新one-hot编码组合$x$-&gt;$w^Tx$-&gt;$P(y=1)=\\sigma(w^Tx)$。</span></p><p><span style=\"font-size: 1rem;\"><span style=\"font-size: 14px;\">根据</span><a href=\"http://www.algorithmdog.com/xgboost-lr-more-feas\" target=\"_blank\"><span style=\"font-size: 14px;\">这个</span></a><span style=\"font-size: 14px;\">blog，原始数据加新造的one-hot编码组合（新特征），效果最好。</span></span></p><p><span style=\"font-size: 1rem;\"><br/></span></p></div>", "lvl2_answer": []}]}