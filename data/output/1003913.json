{"id": "1003913", "question": "最小角回归是天然的LASSO化？正则化参数怎么体现？", "description": "<div class=\"col-md-11 col-xs-10\"><p>和“前向选择（Forward Selection）”和“前向梯度（Forward Stagewise）”算法类似，最小角回归直接面对的问题就是Y=X*w的问题，其中Y是m*1向量，X是m*n矩阵，w是n*1的待求系数向量，如此一来似乎问题和LASSO的损失函数没有关系了，那么说最小角回归求LASSO难道是因为最小角回归天然具备LASSO的特性，能够使得一些系数向量中的值为零？</p><p>那么承认了修正的LARS算法得到的解就是LASSO的解的基础上，LASSO回归中的正则化参数怎么在LARS中得到体现呢？修正的LARS算法只是说当系数转变符号时去掉相应的向量而已？</p></div>", "viewer": 4640, "tags": ["统计/机器学习", "回归分析", "监督式学习", "开放问题"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px;\">很有意思的问题：</span></p><p><span style=\"font-size: 14px;\">1. 这类最小二乘回归问题$argmin_w(|y-w^Tx|^2)$，所求问题是把$var(y)$所代表的$y$能量如何分布到协方差矩阵$Cov(x)=x^Tx$上。$y^Ty=x^Tww^Tx$，其中 </span><span style=\"font-size: 14px;\">$w_i^2$代表$var(x_i)$单独分到能量的比例，$2w_iw_j$代表因为colinear问题$cov(x_i,x_j)$共享能量的比例。</span><span style=\"font-size: 14px;\">举个极端的例子，$y=w_1x_1+w_2x_2$，如果$x_1=x_2$，$corr(x_1,x_2)=1$。</span><span style=\"font-size: 1rem;\"><span style=\"font-size: 14px;\">$y$的能量分给$x_1$或$x_2$ 都一样。 那么 $w_1+w_2=1$都是合理的解。只有加上 $|w|_1$正则项，才可以保证得到$w=[1,0]或[0,1]$这样稀疏的解。换句话说，</span><span style=\"color: rgb(255, 0, 0); font-size: 14px;\">求稀疏解等于能量集中</span><span style=\"font-size: 14px;\">，</span></span><span style=\"font-size: 14px;\">把colinear引起的共享能量集中到一个变量$x_i$上，此时$|w_i|$增大，而$|w_j|$保持较小值。</span></p><p><span style=\"font-size: 14px;\">2.原始的LARS得到稀疏解有三个原因：a)Forward,</span><span style=\"font-size: 14px;\">起始点是原点$w=[0,0,...]$,b)每次只求一个参数， c)boosting，每一步只分配残差的能量。如果$x_i$被选中，会尽量把能量分配给$x_i$,以后剩余在残差中的能量只有很少能分给$x_j$。因为$x_j$一直不能选中，$w_j=0$ ,从而得到稀疏解。</span></p><p><span style=\"font-size: 14px;\">3.如果$w$起始点不是原点，就需要有机制让一部分$|w_j|$减小并停在0。所以Lasso_LARS改进，可以有任意的起始点。</span></p><p><span style=\"font-size: 14px;\">原始LARS有点赢者通吃的想法。比如动物=[狼，猪，羊] , 食物=[肉，草]。如果先让狼去吃，第二个让猪吃草，羊就没有食物 ，此时得到解$[1,1,0]$。如果第二步让羊在猪之前去吃，猪就没食物，解为$[1,0,1]$。如果让猪在狼和羊前去吃，那么狼和羊都没有食物， 解为$[0,2,0]$。</span></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>查阅了一下，应当是使用修正后的LARS算法对LASSO问题进行求解，但是修正算法没怎么看懂，就是把LASSO问题的损失函数变化为一个类似于X*r=Y的形式的时候，Y其实是系数矩阵的1范数对系数矩阵中分量的求导，这个导数是不存在的，这里怎么进行处理呢?若是以次导数的思想来进行，那么每次推进的时候，Y是选-1，+1还是0呢</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>又看了一下，修改后的LARS算法就是将变出现系数变号的向量从解集当中剔除，但是有个问题，损失函数里面的正则化系数怎么在LARS中体现呢？</p></div>", "lvl2_answer": []}]}