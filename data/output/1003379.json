{"id": "1003379", "question": "为什么岭回归和最小二乘支持向量回归有一样二次规划形式却有不同解", "description": "<div class=\"col-md-11 col-xs-10\"><p>岭回归（RS）和最小二乘支持向量回归（LS_SVR）的凸二次回归问题其实是一样的，就是岭回归的正则化参数是乘在惩罚项（权重向量/斜率 的内积）上，而最小二乘支持向量机的正则化参数是乘在误差项上的，在这样的情况下为什么会出现当训练点数大于维度数时岭回归的计算量小于最小二乘支持向量回归呢？\n</p><p>\n</p><p>希望知道关系：<b><i>Y</i></b>=<b><i>X</i></b><sup>T</sup><b><i>ω</i></b>，其中<b><i>X</i></b>是一个N*M矩阵，由M个N维列向量组成</p><p>\n</p><p>\n</p><p>RS:   <b><i>ω</i></b>=(<b><i>XX</i></b><sup>T</sup>+α<b><i>I</i></b>)<sup>-1</sup><b><i>XY\n</i></b></p><p>\n</p><p>\n</p><p>\n</p><p>LS_SVR: 当映射函数φ(<b><i>X</i></b>)=<i><b>X</b></i>时，公式为，<b><i>ω</i></b>=<i><b>X</b></i>(<b><i>X</i></b><sup>T</sup><b><i>X</i></b>+<i>γ</i><sup>-1</sup><i><b>I</b></i>)<sup>-1</sup><b><i>Y</i></b>\n</p><p>\n</p><p>\n</p><p>\n</p><p>可以发现RS需要对一个N*N矩阵求逆，LS_SVR需要对一个M*M矩阵求逆，一般而言M大于N，所以LS_SVR的计算量将显著大于RS的计算量。但是他们的二次规划问题的形式是一样的，只是正则化参数（可以看成一个常系数）乘的位置不一样，怎么会造成最后的求解公式结果不一样呢？难道只是因为求解方法不一样？因为LS_SVR是拉格朗日乘数法而RS是直接对待求权重求偏导？</p></div>", "viewer": 4978, "tags": ["统计/机器学习", "回归分析", "开放问题"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>你提到的</p><p>Ridge</p><p>$$(XX^T+\\alpha I)^{-1}X$$</p><p>和最小二乘SVM回归的</p><p>$$X(X^TX+\\gamma^{-1}I)^{-1}$$</p><p>当$\\alpha=\\gamma^{-1}$的时候，就是完全等价的吧，你把两个逆矩阵分别左乘右乘一下，就得到一样的了</p><p>Ridge</p><p>$$X(X^TX+\\gamma^{-1}I)$$</p><p>LS SVR</p><p>$$(XX^T+\\alpha I)X$$</p></div>", "lvl2_answer": ["不是的，X'X和XX'得到的方阵大小不一样的，而且矩阵乘法没有交换律，针对LS_SVR而言，X在开头的话中间的那个逆不能直接乘到左边去呀", "上面这个答案没有用交换律吧，我觉得没问题", "因为在LS SVR的公式里面还有个Y啊不是么，右乘一个逆会导致不可计算吧，除非使用交换律先计算逆矩阵与原矩阵得到单位阵，但是相当于是（A^(-1)YA）不能写作（A^(-1)AY），而且Y作为一个列向量，（A^(-1)YA）这个式子本身就不能计算吧", "$A=(XX^T+\\alpha I)^{−1}X$，$B=X(X^TX+\\gamma^{−1}I)^{−1}$；如果$A=B$，那么$Ay=By$", "谢谢..我再看看", "多谢哈，隔了几天回来看看懂了，十分谢谢，所以我在想，这两个方法其实结果是一样的，但是涉及到逆矩阵的求解，其中逆矩阵的大小是不一样的，这是不是意味着在映射函数为简单线性的时候，采用RS会更加利于计算"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px;\">当去掉正则项后，从</span><span style=\"color: rgb(255, 0, 0); font-size: 14px;\">数学</span><span style=\"font-size: 14px;\">上二者是等价的。比如$X=USV^T$,</span></p><p><span style=\"font-size: 14px;\">$$w_{RS}=(XX^T)^{-1}XY=(US^{-1}V^TVS^{-1}U^T)USV^TY=US^{-1}V^TY$$</span></p><p><span style=\"font-size: 14px;\">$$w_{LS_SVR}=X(X^TX)^{-1}Y=USV^T(VS^{-1}U^TUS^{-1}V^T)Y=US^{-1}V^TY$$</span></p><p><span style=\"font-size: 14px;\">显然二者相等。</span><span style=\"font-size: 14px;\">但是在实际</span><span style=\"font-size: 14px; color: rgb(255, 0, 0);\">数值计算</span><span style=\"font-size: 1rem;\"><span style=\"font-size: 14px;\">上二者并不相等，因为$XX^T$或$X^TX$可能不可逆，需要加正则项让他们可逆，求出稳定(stable)的逆矩阵解。至于$XX^T$和$X^TX$谁求逆更稳定，理论不太清楚。我的感觉是越小的矩阵越容易求逆。比如$X$是$N\\times M$,如果$N&lt;M$,则应该用RS。</span></span></p></div>", "lvl2_answer": ["十分感谢！"]}]}