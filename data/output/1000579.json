{"id": "1000579", "question": "Random Forest和Tree Bagging什么区别？", "description": "<div class=\"col-md-11 col-xs-10\"><p>Random Forest和Tree Bagging什么区别？</p></div>", "viewer": 9784, "tags": ["统计/机器学习", "监督式学习", "随机森林"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>为了方便叙述，假设有N个样本，P个特征。</p><p><br/></p><p><span style=\"font-weight: bold;\">Tree Bagging是Decision Tree的改良。</span></p><p>对这N个样本有放回的抽样，抽出N个样本作为新的训练集，训练出一个决策树。如此重复m次，得到m个不同的训练集，从而有m个不同的决策树，最后对这m个决策树预测模型取均值。这就是Tree Bagging。</p><p><br/></p><p><span style=\"font-weight: bold;\">Random Forest是Tree Bagging的改良。</span></p><p>对这N个样本有放回的抽样，抽出N个样本作为新的训练集，从P个特征中（无放回）地取出p个特征，训练出一个决策树。如此重复m次，得到m个不同的训练集（特征也不同），从而有m个不同的决策树，最后对这m个决策树预测模型取均值。这就是Random Forest。</p><p><br/></p><p>所以我们可以看出唯一的区别就是Random Forest不仅随机选择了样本，也随机地选择了一些特征。至于p的值是多少，这个可以通过Cross Validation来选择。</p><p><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>简单地说,Tree bagging用了全部特征，RandomForest随机选择了部分特征</p></div>", "lvl2_answer": []}]}