{"id": "1005827", "question": "为什么矩阵的二范数和向量的二范数的定义不同？", "description": "<div class=\"col-md-11 col-xs-10\"><p>矩阵的二范数是最大的singular value，而向量的二范数是元素的平方和的根（就是ridge里的惩罚项），为什么矩阵的二范数和向量的二范数的定义完全不同？名字为什么又一样呢？</p></div>", "viewer": 5487, "tags": ["数学", "线性代数"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>矩阵的二范数是根据向量的二范数的定义引申出来的，矩阵二范数是一种诱导范数(induced norm)。</p><p>长度为$n$向量的$p$-范数的定义是</p><p>$$\\|v\\|_p=\\left(\\sum_{i=1}^n|v_i|^p\\right)^{\\frac{1}{p}}$$</p><p>所以常见的2-范数就是平方和的根，1-范数就是绝对值的和。</p><p>一个$m\\times n$的矩阵的$p$-范数是根据向量的$p$-范数诱导而来，定义如下</p><p>$$\\|A\\|_p := \\max_{v\\in \\mathbb R^n}\\frac{\\|Av\\|_p}{\\|v\\|_p}=\\max_{\\|v\\|_p=1}\\|Av\\|_p$$</p><p>上面式子里$\\|A\\|_p$是矩阵范数，后面的都是向量范数。</p><p>具体来说，对于矩阵2-范数，</p><p>$$\\|A\\|_2 =\\max_{\\|v\\|_2=1}\\|Av\\|_2$$</p><p>我们对$A$进行奇异分解，得到$A=U\\Sigma V^T$，因为$U$和$V$都是酉阵，所以根据向量2-范数的定义，我们有</p><p>$$\\|Av\\|_2^2=\\|U\\Sigma V^Tv\\|_2^2=v^TV\\Sigma^TU^TU\\Sigma V^Tv=v^TV\\Sigma^T\\Sigma V^Tv$$</p><p>把$V^Tv$替换为$u$，得到</p><p>$$\\|Av||_2=\\|\\Sigma u\\|_2$$</p><p>$u$的向量2-范数显然也是等于1，因为$\\|u\\|_2^2=\\|V^Tv\\|_2^2=v^TVV^Tv=\\|v\\|_2^2=1$。</p><p>所以$$\\|A|_2=\\max_{\\|u\\|_2=1}\\|\\Sigma u\\|_2$$</p><p>$\\Sigma$是对角线为奇异值的对角阵，为了使乘积后的二范数最大，只能让$u$为独热向量，唯一的1对应着最大的奇异值。矩阵的2-范数也就是最大奇异值。</p><p><br/></p></div>", "lvl2_answer": ["谢谢！非常详细！"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>矩阵的范数是根据向量的范数诱导的，所以名字一样，定义不同。</p><p>类似的还是1-范数。</p></div>", "lvl2_answer": []}]}