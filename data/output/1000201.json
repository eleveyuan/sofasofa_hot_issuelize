{"id": "1000201", "question": "线性回归或者逻辑回归中常提到的AIC和BIC是什么意思？", "description": "<div class=\"col-md-11 col-xs-10\"><p><span style=\"font-size: 1rem;\">线性回归或者逻辑回归中常提到的AIC和BIC是什么意思？好像和回归模型的变量选择有关。那它们又有什么区别呢？</span><br/></p></div>", "viewer": 28535, "tags": ["统计/机器学习", "回归分析", "数据降维", "特征选择"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px;\">AIC和BIC都是用于同时衡量模型拟合度和复杂度的统计量。换句话说，我们希望利用AIC或者BIC选择出来的模型既有不错的拟合度又不至于太复杂（太多自变量）。</span></p><p><span style=\"font-size: 14px;\">对于一个回归模型，如果优化目标只是最大化log-likelihood函数，即不限制变量系数或变量数量，当然是模型越大（变量越多）模型越精确。考虑到模型计算以及过度拟合，我们又倾向于一个简单的模型。所以我们需要寻找一个相对最优化的模型，来平衡模型大小与模型拟合准确度的关系。选择模型的评估标量有很多，AIC和BIC就是其中的两个。它们的相似处在于都是通过限制模型变量的数量来控制模型的大小，而不同的是惩罚变量数量的函数不同。</span></p><p><span style=\"font-size: 14px;\">具体统计上来说， </span></p><p><span style=\"font-size: 14px;\">$$\\text{AIC}=2p - 2\\log L(\\hat{\\theta}) $$</span></p><p><span style=\"font-size: 14px;\">其中$\\theta$是模型参数，p是模型参数的数量。</span><span style=\"font-size: 14px;\">从模型选择上说，我们要选择AIC最小的那个模型。</span></p><p><span style=\"font-size: 14px;\">$$\\text{BIC}=p\\log(n)-2\\log L(\\hat{\\theta}) $$</span></p><p><span style=\"font-size: 14px;\">其中n是样本数量。</span><span style=\"font-size: 14px;\">类似地，我们优先选择BIC最小的模型。</span></p><p><span style=\"font-size: 14px;\">我们可以注意到，当$1/2\\times \\log(n) &gt; 1$, 也就是，样本数量大于$e^2 = 7.39$时, BIC的复杂度惩罚函数比AIC的惩罚函数大。</span><span style=\"font-size: 14px;\">换句话说，BIC相对于AIC 会选择一个更小的模型。</span></p><p><span style=\"font-size: 14px;\">以上是通过统计角度来简单解释 AIC和BIC。在信息论角度，我们也可以用最小描述长度原则来解释这两个评估值</span><span style=\"font-size: 14px;\">。</span></p><p><span style=\"font-size: 14px;\">p.s. BIC的概念来源于贝叶斯，有知道的童鞋可以解释一下为什么吗？</span></p><p><span style=\"font-size: 14px;\"><br/></span></p></div>", "lvl2_answer": []}]}