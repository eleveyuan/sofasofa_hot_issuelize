{"id": "1000433", "question": "hashing trick或者feature hashing是什么", "description": "<div class=\"col-md-11 col-xs-10\"><p>我看到有人说可以用hashing trick来进行降维？这个hashing trick是什么意思？怎么降维？</p><p>好像feature hashing也是这个意思。</p><p><br/></p></div>", "viewer": 9737, "tags": ["统计/机器学习", "数据预处理", "数据降维"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>Hashing trick，有时候也叫做feature hashing，在自然语音中已经用作降维的手段。在一般的机器学习任务中，它也可以对categorical feature进行降维。</p><p>举个例子，比如你是淘宝的算法工程师，你要做一个退货的预测模型，假设有一个feature是location_id，表示商品的产地。这个是categorical feature，所以你通常需要做one-hot encoding，把这一列转化为dummy variable。商品来自全国各市、全球各国，可能这个location_id就有成千上万个数值。转码之后，模型就会增加这一万个dummy变量。这对数据的读取、操作，模型的训练都是极大的挑战。</p><p>Hashing trick就是用hashing function这个小技巧来降维。若location_id都是整数，我们可以对所有的location_id取余，location_id (mod p)，这个取余函数就是我们使用的hashing function。很显然进行取余操作之后，我们最多只有p个不同的数值了。在此之上再用one-hot encoding，我们只增加了p列。</p><p>location_id        location_id (mod 5)</p><p>21                        1</p><p>9126                    1</p><p>45                        0</p><p>10                        0</p><p>1189                    4</p><p>Hashing trick有三个主要的优点</p><p>1.降维程度大</p><p>2.计算快速、方便</p><p>3. 不要额外的存储空间（额外的参考词典等）</p><p>但是，也有些缺点。比如我们观察到上面产地编号9126和21除以5的余数都是1，它们就被放到了一起。在Hashing trick中，这种冲突和合并是无法避免的。但是根据一些论文和大量业界应用的结果，这种冲突合并对预测模型的表现的影响微乎其微。另一个缺点，因为大量的数值并合并，这使得模型和结果不易interpret。</p></div>", "lvl2_answer": []}]}