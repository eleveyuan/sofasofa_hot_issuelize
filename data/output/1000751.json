{"id": "1000751", "question": "bias-variance tradeoff是什么意思？", "description": "<div class=\"col-md-11 col-xs-10\"><p>很多模型都提到bias-variance tradeoff，这个到底怎么理解？</p></div>", "viewer": 7123, "tags": ["统计/机器学习", "开放问题"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>bias(偏差)一般都是由于模型中错误的前提假设导致的。高bias通常意味着模型欠拟合。</p><p>variance(方差)一般是由于模型对数据集中的噪音的敏感导致的。高variance通常意味着模型过拟合。</p><p>bias–variance decomposition（偏差-方差分解）是将模型中的误差(error)分解成bias和variance两部分</p><p>$$\\text{Error}=\\text{Error}_\\text{bias}+\\text{Error}_\\text{var}$$</p><p>很明显，如果在Error不变的情况下，如果降低$\\text{Error}_\\text{bias}$，必然会导致$\\text{Error}_\\text{var}$变大；反之亦然。这就是所谓的bias-variance tradeoff。</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>我们在建立模型的时候，希望这个模型又要精度高，又要方差小，但是通常很难两全其美。所以我们要在这两者之间权衡。</p><p><br/></p><p>比如说一个人找两个大师去算命。大师A说，你在20岁到80岁之间会发一次大财，这是你这辈子最大的一次财运；大师B说，你在41、2岁的时候会发一次大财，这是你这辈子最大的一次财运。过了几十年之后，你在45岁的时候发了人生一笔横财。那么回过头想，哪个大师说得准呢？</p><p><br/></p><p>回到机器学习上面，举个例子。对于kNN来说，k选的小，可以让bias变小，但是variance会变大；k选大了，bias会大，但是variance会变小。所以k过大，k过小都不好，就像上面的大师A和B。让bias和variance达到平衡，这样的模型才比较好。</p><p><br/></p><p>这个英文博客有兴趣的话可以看看<a href=\"http://scott.fortmann-roe.com/docs/BiasVariance.html\">bias variance tradeoff</a>。</p><p><br/></p></div>", "lvl2_answer": []}]}