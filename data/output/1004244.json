{"id": "1004244", "question": "为什么LR要用Sigmoid函数？", "description": "<div class=\"col-md-11 col-xs-10\"><p>如题，面试的时候被问到了...</p></div>", "viewer": 6741, "tags": ["统计/机器学习", "监督式学习"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>Sigmoid是逻辑回归作为glm的link函数。之所以用它是因为：</p><p>1. 线性模型的输出都是在$[-\\infty, +\\infty]$之间的，而Sigmoid能够把它映射到$[0,1]$之间。正好这个是概率的范围。</p><p>2. Sigmoid是连续光滑的。</p><p>3. 根据Sigmoid函数，最后推导下来逻辑回归其实就是最大熵模型，根据最大似然估计得到的模型的损失函数就是logloss。这让整个逻辑回归都有理可据。</p><p>4. Sigmoid也让逻辑回归的损失函数成为凸函数，这也是很好的性质。</p><p>5. 逻辑回归的损失函数是二元分类的良好代理函数，这个也是Sigmoid的功劳。</p><p>至于<a href=\"http://sofasofa.io/forum_main_post.php?postid=1001792\" target=\"_blank\">为什么LR不用MSE，看这里</a>。</p></div>", "lvl2_answer": ["赞！"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px;\">Sigmoid就是有广义线性模型GLM推出来的。对于分类问题，GLM的基本思想是把输入$X$的线性组合($score_k(x)=\\beta_kx$)通过逆<a href=\"https://en.wikipedia.org/wiki/Generalized_linear_model\" target=\"_blank\">link function</a>映射到非线性的概率分布$P(Y=k|x)=P_k(x)=\\Phi(\\beta_kx)$。$\\Phi$是link function的逆函数，有两种选择：一种是logistic function/Sigmoid，是取自然指数后再归一化，</span><span style=\"font-size: 14px;\"> $P_k(x)=e^{\\beta_kx}/Z$，得到logistic regression，残差是<a href=\"https://en.wikipedia.org/wiki/Logistic_distribution\" target=\"_blank\">logistic分布</a>；还有一种是正态分布的CDF，得到Probit model，残差是正态分布。</span></p><p><span style=\"font-size: 14px;\">对于二元分类，$Z=e^{\\beta_0x}+e^{\\beta_1x}$,归一化后只有一组参数$\\beta$是独立的：</span></p><p><span style=\"font-size: 14px;\">$$P_1(x)=\\frac{e^{\\beta_1x}}{e^{\\beta_1x}+e^{\\beta_0x}}$$</span></p><p><span style=\"font-size: 14px;\">$$=\\frac{1}{1+e^{-(\\beta_1-\\beta_0)x}}$$</span></p><p><span style=\"font-size: 14px;\">$$=\\frac{1}{1+e^{-\\beta x}}$$</span></p><p><span style=\"font-size: 14px;\">Sigmoid就是softmax 在K=2的特例。</span></p><p><span style=\"font-size: 14px;\">然后是假设数据点都服从独立的伯努利分布，建立似然函数，再用最大似然法求$\\beta$，这些都和Sigmoid无关。</span></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>我自己补充一下吧 不知正确</p><p>是广义线性模型理论和伯努利分布推导出的函数形式和sigmoid函数相同。</p><p>具体可以看这个链接<a href=\"https://www.cnblogs.com/zhangyuhang3/p/6873339.html\">https://www.cnblogs.com/zhangyuhang3/p/6873339.html</a><a href=\"https://www.cnblogs.com/zhangyuhang3/p/6873339.html\" target=\"_blank\"></a></p><p>但为啥推导出了h(x)函数形式,就要用这个形式，还没有理解。</p><p><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>因为交叉熵损失函数只和分类正确的预测结果有关。而平方损失函数还和错误的分类有关，该损失函数除了让正确分类尽量变大，还会让错误分类都变得更加平均，但实际中后面的这个调整使没必要的。但是对于回归问题这样的考虑就显得重要了，因而回归问题上使用交叉熵并不适合。</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>sigmoid函数输出0,1之间，训练好可模拟概率实现分类</p><p>&gt;0.5判定为正例，反之为反例（也是新手，说说自己的理解）</p><p>导数好求</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>欢迎关注SofaSofa的面试题库哦，这个问题已经在<a href=\"http://sofasofa.io/interviews/J44.php\" target=\"_blank\">卷44: 机器学习</a>中收录了哦。</p><p>祝您面试顺利！</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>二项分布对应的就是sigmoid，多项分布对应的是softmax</p></div>", "lvl2_answer": []}]}