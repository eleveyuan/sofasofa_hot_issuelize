{"id": "1655929", "question": "主成分分析法(PCA)算是黑盒算法吗？", "description": "<div class=\"col-md-11 col-xs-10\">才接触机器学习不大久，了解到主成分分析，是线性降维的方法，但是降维之后的特征和原来的特征好像就不大对的上了。那么PCA算是黑盒算法吗？</div>", "viewer": 129, "tags": ["统计/机器学习", "无监督学习", "数据降维"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>PCA这种降维方式是把原来的m个变量，进行糅杂，缩减到n个新组建的变量上。至于新组建的变量又是原变量的线性组合。线性组合本身具有可解释性，但是这些变量硬组合在一起，基本上是无法理解的。我觉得是类似黑盒的。</p><p>举个例子，比如说对一个商品数据进行pca降维，其中一个新变量是 2*商品价格 - 0.5*商品重量 + 1.2*商品销量；这种情况下，就完全没有可解释性了。</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>特征选择是从已存在的特征中选取携带信息最多的，选完之后的特征依然具有可解释性，我们依然知道这个特征在原数据的哪个位置，代表着原数据上的什么含义。</p><p>PCA是降维算法，将已存在的特征进行压缩，降维完毕后的特征不是原本的特征矩阵中的任何一个特征，而是通过某些方式组合起来的新特征。通常来说，在新的特征矩阵生成之前，我们无法知晓降维算法们都建立了怎样的新特征向量，新特征矩阵生成之后也不具有可读性。</p><p>PCA一般不适用于探索特征和标签之间的关系的模型（如线性回归），因为无法解释的新特征和标签之间的关系不具有意义。在线性回归模型中，我们更倾向于用特征选择。</p><p>\n</p><p>\n</p></div>", "lvl2_answer": []}]}