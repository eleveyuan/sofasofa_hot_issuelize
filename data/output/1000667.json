{"id": "1000667", "question": "对于小批量随机剃度下降法(mini-batch SGD)，如何选择每批样本的数量？", "description": "<div class=\"col-md-11 col-xs-10\"><p>随机剃度下降法是每次使用一个样本，小批量随机剃度下降是每次使用m个样本。这个m一般怎么选择？有什么技巧？</p></div>", "viewer": 6486, "tags": ["数学", "数值计算", "最优化"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px;\">这篇论文</span><a href=\"https://arxiv.org/abs/1609.04836\" target=\"_blank\"><span style=\"font-size: 14px;\">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</span></a><span style=\"font-size: 14px;\">给出了关于mini-batch样本点的一些观察：</span></p><p><span style=\"font-size: 14px;\">1. 样本点太少，训练时间长；样本点太多，训练时间也太长。</span></p><p><span style=\"font-size: 14px;\">2. 样本点多，训练单个epoch时间更短</span></p><p><span style=\"font-size: 14px;\">3. 样本点越小，模型的泛化越好。</span></p><p><br/></p><p><span style=\"font-size: 14px;\">理论归理论，实际上还是自己选一些比较小的数值，</span><span style=\"font-size: 14px;\">比如8，12，32，64。</span></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>一般来说是16到256之间。</p></div>", "lvl2_answer": []}]}