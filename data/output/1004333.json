{"id": "1004333", "question": "为什么说LR适合大样本，SVM适合小样本？", "description": "<div class=\"col-md-11 col-xs-10\"><p>谢谢</p></div>", "viewer": 8800, "tags": ["统计/机器学习"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>数据矩阵$X$是$d \\times n$，$n$为数据点数，$d$为维数。先定义$d \\ll n$是大样本，$n \\ll d$是小样本。$d \\approx n$时，LR和SVM都可用。LR(逻辑回归)有$d$个待求的特征权重$w$，SVM有 $n$个数据点权重$\\alpha$。</p><p>1. SVM只需要较少的支持向量（权重$\\alpha&gt;0$的点）就可以确定分割超平面，而对于LR，当$n&lt;10d$时，有可能会有过拟合问题。</p><p>2. 再比较下LR和SVM的训练的计算和空间复杂度。</p><p>假定用gradient descent解LR，用Coordinate desent解SVM的dual probelm，它们主要计算量在求梯度。</p><p>对于LR，梯度$df/dw=X(h(X^Tw)-y)$, $h()$是sigmiod函数，<a href=\"http://sofasofa.io/forum_main_post.php?postid=1002656\" target=\"_blank\">参考</a>。计算复杂度$\\mathcal{O}(ndT)$,$T$是循环次数。</p><p>对于SVM， 增量$\\delta_i^*=\\frac{1-(Q\\alpha)_i}{Q_{ii}}$,$Q_{ij}=y_iy_jk(x_i,x_j)$,<a href=\"http://www.stat.ucdavis.edu/~chohsieh/teaching/ECS289G_Fall2015/lecture6.pdf\" target=\"_blank\">参考</a>。计算复杂度$\\mathcal{O}(n^2T)$。如果需要存储kernel distance matrix$K$，空间复杂度是$\\mathcal{O}(n^2)$。</p><p>3. 应该反过来说，因为过拟合，小样本不适合LR，需要降维；因为计算和空间复杂度，大样本不适合SVM，需要预处理，减小样本数。</p><p>4.zl_pku提到LR可以online training，SVM也可以online。比如找出的support vectors加新数据再算SVM。</p><p><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>SVM不适合大样本，因为计算的原因，SVM会非常慢。SVM适合小样本是因为SVM只由支持向量决定，所以样本多少对最终的支持向量影响不大。</p><p>LogisticRegression适合大样本是因为我们可以用在线数值方法进行迭代求解，比如随机梯度下降，这样速度很快也非常适合并行计算。但是LogisticRegression也同样适合小样本，只要满足样本数量是变量数量的10倍以上就够了。</p></div>", "lvl2_answer": []}]}