{"id": "1002508", "question": "数据归一化问题", "description": "<div class=\"col-md-11 col-xs-10\"><p><span style=\"font-size: 1rem;\">如果输入数据进行了归一化操作，经过网络模型，得到的预测输出值是不是也会偏小；意思是输入数据归一化，会影响预测的输出值？这时候是不是标签数据也需要进行归一化操作？</span><br/></p><p>\n</p><p><br/></p></div>", "viewer": 4159, "tags": ["统计/机器学习", "监督式学习", "深度学习"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>数据归一化（normalization）相当于做数据矩阵的较弱的right preconditioning，目的是减小矩阵的condition number，从而让梯度递减一类的优化算法收敛更快，而且求矩阵逆的误差减小。</p><p>举个简单的例子，一层的线性回归，$Xw=y$，其中$X$是mxn输入数据矩阵，每一行是一个n维数据点，$w$是要求的网络参数，y是m个1维输出。<span style=\"font-size: 14px;\">当做数据归一化时，相当于乘上了right preconditioner$P$。</span></p><p><span style=\"font-size: 14px;\">$XP^{-1}Pw=y$</span></p><p><span style=\"font-size: 14px;\">其中$P$是个简单的可逆的对角线矩阵。$P_{ii}=\\sigma_i$, $P_{ij}=0,i \\neq j$, $\\sigma_i$是输入数据X第i维的标准差。新求的网络参数是$Pw=w'$。新矩阵$XP^{-1}=X'$的condition number会小于$X$。还可以看出对$y$没有变化。</span></p><p><span style=\"font-size: 14px;\">当用最小二乘法时，$loss=argmin_w(|XP^{-1}w'-y|^2)$，近似锥形。当condition number约等于1时，loss function在$w'$空间上的等高线（contour）更接近于圆/球；否则，当condition number是一个很大的正数时，等高线更接近于椭圆/椭球。对于梯度下降算法，圆形等高线下降更快。</span></p><p><span style=\"font-size: 1rem;\"><span style=\"font-size: 14px;\">结论是应该对输入数据做归一化，保持原始输出。</span><br/></span><br/></p></div>", "lvl2_answer": ["normalization是数据正则化吧？归一化和正则化不是一个东西吧？", "正则化是regularization吧。normalization可以说是归一化，也可以说是正规化。", "归一化，标准化，正规化，正则化这几个词被用混了。\n我理解的归一化是rescaling，把值变为0,1之间。标准化（normalization）也可叫正规化（standarization），指z-score normalization。正则化（regularization）指给损失函数（loss function）中加入正则项（regularization term），比如L1/L2/total variation。"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>归一化是把数据都变为（0,1）的范围内，这样的话处理起来方便，计算量也大大减少了，损失函数更容易收敛。<br/></p></div>", "lvl2_answer": []}]}