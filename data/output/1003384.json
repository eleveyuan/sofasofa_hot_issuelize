{"id": "1003384", "question": "BP神经网络的疑问", "description": "<div class=\"col-md-11 col-xs-10\"><p>权值更新的过程是这样的吗？</p><p>假设输入层有2个节点，隐藏层有1层2个结点，输出层只有1个结点。（这样就有2*2+2=6个权值）</p><p>第一次训练</p><p>    样本1输入，更新权重w1-w6</p><p>    样本2输入，基于上次更新的w1-w6再次更新w1-w6</p><p>    ...</p><p>    样本n输入，基于上次更新的w1-w6再次更新w1-w6</p><p>第二次训练</p><p>    样本1输入，基于第一次训练最后一次更新的w1-w6再次更新w1-w6</p><p>    样本2输入，基于上次更新的w1-w6再次更新w1-w6</p><p>    ...</p><p>    样本n输入，基于上次更新的w1-w6再次更新w1-w6</p><p>...</p><p>第N次训练</p><p>    样本1输入，基于第N-1次训练最后一次更新的w1-w6再次更新w1-w6\n</p><p>    样本2输入，基于上次更新的w1-w6再次更新w1-w6\n</p><p>    ...\n</p><p>    样本n输入，基于上次更新的w1-w6再次更新w1-w6</p><p><br/></p><p>是这样的过程吗？</p><p><br/></p></div>", "viewer": 2776, "tags": ["统计/机器学习", "人工神经网络"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>你描述的这个过程其实就是广义的mini-batch的训练过程，跟是否是bp或者是否是神经网络都没有关系。你一共重复训练了N次，也就是说epoch是N。</p><p>backpropagation神经网络也是可以这样训练。</p><p>bp的意思是先向前传播，再反向传播更新权重。</p></div>", "lvl2_answer": ["sgd：每次训练权重更新次数为N次\nmini-batch sgd：每次训练权重更新次数为N/b次\ngd：每次训练权重更新次数为1次\n是这样的吗？", "是的，可以这么理解"]}]}