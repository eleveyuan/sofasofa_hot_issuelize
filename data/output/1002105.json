{"id": "1002105", "question": "python中计算KL divergence", "description": "<div class=\"col-md-11 col-xs-10\"><p>python中计算KL divergence有什么直接调用的函数吗？</p><p>如果没有的话，那就自己手写咯</p><p><br/></p></div>", "viewer": 4920, "tags": ["统计/机器学习", "概率分布", "描述性统计", "Python"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html\" target=\"_blank\">scipy里的entropy</a>实际上就可以计算Kullback–Leibler divergence</p><pre><code class=\"python\">&gt;&gt;&gt; from scipy.stats import entropy\r\n&gt;&gt;&gt; entropy(p, q)</code></pre><p><br/></p></div>", "lvl2_answer": ["这个是二项分布的情况"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>@sasa 写了二项分布的情况，其实KL还可以做两个高斯分布的情况</p><p>假设两个高斯分布$p:N(\\mu_1, \\sigma_1^2)$，$q:N(\\mu_2,\\sigma_2^2)$，这两个高斯分布的KL divergence就等于</p><p>$$\\text{KL}(p,q)=\\log\\frac{\\sigma_2}{\\sigma_1}+\\frac{\\sigma_1^2+(\\mu_1-\\mu_2)^2}{2\\sigma_2^2}-\\frac{1}{2}$$</p><p>上面的式子用python很好写的</p></div>", "lvl2_answer": []}]}