{"id": "1002251", "question": "KL divergence为什么不是对称的？", "description": "<div class=\"col-md-11 col-xs-10\"><p>之前我问了<a href=\"http://sofasofa.io/forum_main_post.php?postid=1002105\">python中计算KL divergence</a>，但是发现KL divergence不是对称的，比如说KL(a, b) != KL(b, a)。</p><p>为什么不对称呢？是两者的意义不同吗？</p><p><br/></p></div>", "viewer": 6902, "tags": ["统计/机器学习", "概率分布", "描述性统计"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px;\">KL divergence又可以叫相对熵</span></p><p><span style=\"font-size: 14px;\">从它的计算过程：</span></p><p><span style=\"font-size: 14px;\">$$KL(p||q)=-\\int p(x)\\ln q(x)\\mathrm{d}x-\\left ( -\\int p(x)\\ln p(x)\\mathrm{d}x \\right )$$</span></p><p><span style=\"font-size: 14px;\">可以看到不满足对称。</span></p><p><span style=\"font-size: 1rem;\"><span style=\"font-size: 14px;\">信息论里可以理解右边一项是x分布为p(x)时，传输x信息的</span><span style=\"background-color: rgb(255, 255, 0); font-size: 14px;\">最小编码长度</span><span style=\"font-size: 14px;\">。左边一项是x的分布为p(x)，却被误认为分布是q(x)情况下的</span><span style=\"background-color: rgb(255, 255, 0); font-size: 14px;\">实际编码长度</span><span style=\"font-size: 14px;\">。所以非负性也可以理解。</span></span></p><p><span style=\"font-size: 1rem;\"><br/></span></p><p><span style=\"font-size: 1rem;\"><br/></span></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>从信息论的角度来说，$KL(P||Q)$是指当我们用$Q$去逼近$P$的信息损失量。</p><p><br/></p><p>从统计角度来说可能更好理解一点。两个概率分布$P$和$Q$，概率密度函数分别为$p(x)$和$q(x)$。</p><p>我们根据概率分布$P$去生成一个随机点$x$，那么这个$x$有多大的可能性是属于概率分布$Q$的呢？它们的似然函数分别为$p(x)$和$q(x)$。因为$P$是<b>所谓的原分布</b>，那么$q(x)$越接近$p(x)$，所以$\\frac{q(x)}{p(x)}$就该接近1。</p><p>顺理成章，就很好理解$KL$为什么不对称了。$KL(P||Q)$指的是$P$为原分布，$KL(Q||P)$指的是$Q$为原分布。</p><p>$$KL(P||Q)=\\lim_{n\\rightarrow\\infty} \\frac{1}{n}\\sum_{i=1}^n\\log\\frac{{q(x_i)}}{p(x_i)}$$</p><p><br/></p><p>此外英文好得可以看看<a href=\"https://medium.com/@cotra.marko/making-sense-of-the-kullback-leibler-kl-divergence-b0d57ee10e0a\" target=\"_blank\">这篇文章</a>，写得更具体。<br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>如果题主需要一个具有对称性的度量的话，推荐<a href=\"https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\" target=\"_blank\">Jensen–Shannon divergence</a>，它可以看作是对称版的Kullback–Leibler divergence。</p><p><br/></p></div>", "lvl2_answer": []}]}