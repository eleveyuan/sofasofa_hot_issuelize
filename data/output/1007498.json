{"id": "1007498", "question": "在数据很大的场景下怎么进行数据探索？", "description": "<div class=\"col-md-11 col-xs-10\"><p>如在使用spark的情况下，数据一共有2亿条。是运用collect转换成如python可处理的格式后进行操作吗？还是有更好的方法进行数据探索。</p></div>", "viewer": 1257, "tags": ["统计/机器学习", "开放问题", "应用场景"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>你应该不需要直接处理到2亿条数据的吧</p><p>如果是要用2亿条数据直接建模，你可以用sparkmllib</p><p>如果你只是去做一些交叉分析然后做一些可视化，可以先在spark里进行筛选汇总操作，最后再pandas出来一个很小的dataframe</p></div>", "lvl2_answer": ["谢谢你的回答，应该说具体场景是类似于建模前需要对数据分布等进行绘图。比如说欺诈分析是需要进行绘图来查看两者分布的。我理解你的意思是比如做直方图，会先对变量值按值分组，count，接下来把这个小的部分给转换成pd.dataframe后作图，请问理解得对吗？", "是的。如果你数据特别大，也可以先做随机采样再做直方图，会节约很多时间。", "也可以直接在spark里得到直方图的分布数据，然后再导入到本地用matplotlib画出来"]}]}