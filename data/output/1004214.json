{"id": "1004214", "question": "怎么理解所谓的dying relu？", "description": "<div class=\"col-md-11 col-xs-10\"><p>印象中一般都是说sigmoid会在两端梯度消失，dying relu这个该怎么理解？</p></div>", "viewer": 3728, "tags": ["统计/机器学习", "人工神经网络"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px;\">假设一个神经元(neuron)是$Relu(wx+b)$。</span><span style=\"font-size: 14px;\">因为一般用mini batch（SGD）优化算法，每次计算gradient只用一组（batch）数据点。假如用一组数据点更新$w,b$后，</span><span style=\"font-size: 1rem;\"><span style=\"font-size: 14px;\">其余数据点$wx+b&lt;0$，那么只会有一组点能通过这个neuron并更新它的参数，对于绝大多数点来说，不能通过这个neuron，也不能更新参数，相当于“死掉”。如果dying </span><span style=\"font-size: 14px;\">relu</span><span style=\"font-size: 14px;\"> 很多，对于大多数数据来说神经网络大部分通路断掉，学习能力就变弱了。</span></span></p><p><span style=\"font-size: 14px;\">解决办法是用leakyRelu等；bias的初始值设为正数，比如1；减小learning rate。</span></p></div>", "lvl2_answer": []}]}