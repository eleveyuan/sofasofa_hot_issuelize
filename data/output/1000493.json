{"id": "1000493", "question": "sklearn cross_val_score中的参数pre_dispatch", "description": "<div class=\"col-md-11 col-xs-10\"><p>我用sklearn中的cross_val_score对classifier进行交叉验证，我设置的是12-fold，n_jobs=3，结果内存爆掉了。我查了一下，cross_val_score里的pre_dispatch或许能够解决内存不足的问题。但是反复看<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\" target=\"_blank\">cross_val_score documentation</a>，没看懂这个pre_dispatch到底是什么意思，怎么用。</p><p>有人设置过这个参数吗？到底怎么设置？</p><p>谢谢！</p></div>", "viewer": 7218, "tags": ["统计/机器学习", "模型验证", "Python"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>你的设置是cv=12, n_jobs=3，也就是用三个处理器（记为CPU_A, CPU_B, CPU_C），进行12次cross validation（记为CV_1, CV_2, CV_3, ..., CV_12）。pre_dispatch是预派遣的意思，就是提前先把任务派遣给各个处理器。</p><p><br/></p><p>如果我们没有设置cross_val_score中的参数pre_dispatch，当我们开始执行cross_val_score，程序会一次性把全部12个CV都派遣出去，每个处理器领到4个CV。要特别注意了，这里的派遣并不是口头的安排任务，而是把任务和任务对应的数据(划重点)也发送给处理器。比如说，CPU_A领到了CV_1, CV_4, CV_7, CV_10，那么CPU_A就领到了四份训练数据集、四份测试集存放在内存里（又是重点），然后CPU_A开始依次完成CV_1, CV_4, CV_7, CV_10。</p><p><br/></p><p>如果我们设置pre_dispatch=‘2*n_jobs’，当我们开始执行cross_val_score，程序会派遣6个CV出去，每个处理器领到2个CV。所以一开始每个处理器只需要存两份训练集、测试集（划重点）。比如说CPU_A领到了CV_1和CV_4，CPU_B领到了CV_2和CV_5，CPU_C领到了CV_3和CV_6，如果CPU_B率先完成了CV_2，那么系统会自动把CV_7派遣给CPU_B，节奏CPU_A完成了CV_1，系统再把任务CV_8放在CPU_A的任务队列里。pre_dispatch=‘2*n_jobs’的意思就是保持每个CPU都有两个任务在身（一个在做，一个在排队），除非所有任务都被派遣出去了。</p><p><br/></p><p>如果我们设置pre_dispatch=‘1*n_jobs’，这样占用的内存最低，因为只有当当前的任务完成之后，才会有新任务（数据）派遣到闲置处理器，而非将数据放在队列中等待。</p></div>", "lvl2_answer": []}]}