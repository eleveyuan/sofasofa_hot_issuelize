{"id": "1003937", "question": "怎么利用permutation importance来解释xgboost模型的特征？", "description": "<div class=\"col-md-11 col-xs-10\"><p>xgboost模型一般比较难解释特征的重要性，一个是用xgboost自带的feature importance。另外一个方法是用permutation importance。</p><p>请问permutation importance是怎么计算的？怎么来判断特征的重要性的？</p></div>", "viewer": 10441, "tags": ["统计/机器学习", "监督式学习", "特征选择"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>permutation importance是用来衡量特征的重要性的，不光是xgboost，对于很多模型有可以用，可以是分类也可以是回归。</p><p>步骤很简单：</p><p>    <b>1. 在数据集A上训练一个xgboost模型</b></p><p>    <b>2. 在数据集B上测试这个模型，得到MSE（回归）或者logloss（分类，或者auc）</b></p><p><b>    3. 对数据集B中的某一个特征中的值打乱（随机置换，random permute），再用模型进行预测，得到新的指标，这个指标在步骤2和3之间的差值，就是这个特征的重要性</b></p><p>显然差值越大，说明被打乱对模型预测能力影响越大，也是特征越重要。</p></div>", "lvl2_answer": ["谢谢，很有用"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>你提到的permutation importance来自这篇文章<a href=\"http://Permutation importance: a corrected feature importance measure\" target=\"_blank\">Permutation importance: a corrected feature importance measure</a></p><p>Kaggle上有用lightgbm实现的版本：</p><p><a href=\"https://www.kaggle.com/ogrellier/feature-selection-with-null-importances\">https://www.kaggle.com/ogrellier/feature-selection-with-null-importances</a><a href=\"https://www.kaggle.com/ogrellier/feature-selection-with-null-importances\" target=\"_blank\"></a></p><p><a href=\"https://www.kaggle.com/ogrellier/feature-selection-target-permutations\" target=\"_blank\">https://www.kaggle.com/ogrellier/feature-selection-target-permutations</a><br/><br/></p></div>", "lvl2_answer": ["第一个链接失效了"]}]}