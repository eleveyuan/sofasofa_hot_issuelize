{"id": "1003603", "question": "学习率不当会导致sgd不收敛吗？", "description": "<div class=\"col-md-11 col-xs-10\"><p>我想知道学习率不当会导致sgd不收敛吗？还是只是会导致收敛慢？</p></div>", "viewer": 5809, "tags": ["数学", "数值计算", "最优化"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>是有可能的。比如我们有梯度下降求解$f(x)=x^4$的最小值。</p><p>假设初始点是$x=2$，学习率（步长）是$0.5$</p><p>初始的时候</p><p>$x = 2$, $f'(x) = 32$, $f(x) = 16$</p><p>经过一次迭代</p><p>$x = -14.0$, $f'(x) = -10976$, $f(x) = 38416$</p><p>又一次迭代</p><p>$x = 5474$, $f'(x) = 656106545696$, $f(x) = 897881807784976$</p><p>我们看到$x$在来回摆荡，而且离最小值越来越远，<b>显然这个情况下就是因为学习率太大了，导致每次更新时都“过犹不及”“矫枉过正”，所以最后并没有收敛。</b></p><p><br/></p><p>如果学习率是$0.1$，</p><p>$x = 2 $, $f'(x) = 32 $, $f(x) = 16 $</p><p>$x = -1.2 $, $f'(x) = -6.91 $, $f(x) = 2.07$</p><p>$x = -0.51$, $f'(x) = -0.53$, $f(x) = 0.06$</p><p>$x = -0.46$, $f'(x) = -0.38$, $f(x) = 0.04$</p><p>$x = -0.42$, $f'(x) = -0.29$, $f(x) = 0.03$</p><p>$x = -0.39$, $f'(x) = -0.24$, $f(x) = 0.02$</p><p>我们就看到是在逐渐收敛的了</p></div>", "lvl2_answer": ["谢谢你的例子"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>举一个例子说明学习率足够小，可以保证得到收敛的解。</p><p>解$Ax=b$, $A$可做SVD，$A=USV^T$。</p><p>Gradient descent解为</p><p>$$x_n=x_{n-1}-\\alpha A^T(Ax_{n-1}-b)$$</p><p>$$=(I-\\alpha A^TA)x_{n-1}+\\alpha A^Tb$$</p><p>为简化计算，把$x,A,b$都投影到singluar value的向量上，$Z=V^Tx, C=U^Tb$</p><p>$$VZ_n=V(I-\\alpha S^2)Z_{n-1}+V\\alpha SC$$</p><p>令$B=I-\\alpha S^2$</p><p>$$Z_n=BZ_{n-1}+\\alpha SC$$</p><p>$$Z_n=B^nZ_0+(\\sum_{k=0}^{n}B^n)\\alpha SC$$</p><p>令$Z_0=0$</p><p>$$Z_n=(\\sum_{k=0}^{n}B^n)\\alpha SC$$</p><p>根据<a href=\"https://en.wikipedia.org/wiki/Neumann_series\" target=\"_blank\">Neumann series</a>，$(I-B)^{-1}=\\sum_{k=0}^{\\infty}B^k$</p><p>$$\\lim_{n\\to \\infty}Z_n=(I-B)^{-1}\\alpha SC$$</p><p>$$=1/\\alpha S^{-2}\\alpha SC$$</p><p>$$=S^{-1}C$$</p><p>$$\\lim_{n\\to \\infty}x_n=VS^{-1}U^Tb$$</p><p>Neumann series收敛的条件是$B$的最大singular value小于1，$|B|&lt;1 =&gt; \\alpha S_1^2-1&lt;1 =&gt;\\alpha&lt;2/S_1^2$。当学习率足够小，$\\alpha&lt;2/S_1^2$，保证收敛。</p><p>从控制论看，$-1&lt;1-\\alpha S_1^2&lt;0$时欠阻尼，振荡衰减；$0&lt;1-\\alpha S_1^2&lt;1$时，过阻尼，无振荡衰减。</p></div>", "lvl2_answer": ["这么说还是学习率小点稳妥"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>会，当你的学习率设置的过大的时候，就有可能发生不收敛的情况。</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>学习率太大就会overshoot(超调)，就是一下子冲得过多；太小的话收敛太慢，或者陷入局部最优</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>学习过大，算法发散</p></div>", "lvl2_answer": []}]}