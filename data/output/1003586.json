{"id": "1003586", "question": "Python tf神经网络二分类，最后结果都是一样的", "description": "<div class=\"col-md-11 col-xs-10\"><p>神经网络input是140个样本 1296维度的特征值，label前60是[1.0, 0.0] 后面是[0.0, 1.0]，想来实现二分类，可是我用softmax交叉熵+L2正则化，训练出来的output全是一个相同的1*2矩阵。</p><p><br/></p><p>有没有大神可以看出来错在哪里  T_T</p><pre><code class=\"python\">    xs = tf.placeholder(tf.float32,shape=(None, 36*36))\n    ys = tf.placeholder(tf.float32,shape=(None,2))\n\n    layer1_node = 150 # 第一层隐含层神经元个数\n\n    # first layer inference\n    L1_Weights = tf.Variable(tf.random_normal([36*36,layer1_node], stddev=0.1, seed=1))\n    L1_biases = tf.Variable(tf.zeros([1,layer1_node]) + 0.1)\n    L1_out = tf.matmul(xs, L1_Weights) + L1_biases\n\n    L1_out = tf.nn.relu(L1_out)\n\n    # second layer inference\n    L2_Weights = tf.Variable(tf.random_normal([layer1_node,2], stddev=0.1, seed=1))\n    L2_biases = tf.Variable(tf.zeros([1,2]) + 0.1)\n    out = tf.matmul(L1_out, L2_Weights) + L2_biases\n\n    # cross entropy\n    out_pro = tf.nn.log_softmax(out)\n    cross_entropy = -tf.reduce_sum(ys*out_pro)\n    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n\n    # regularization\n    regularizer = tf.contrib.layers.l2_regularizer(0.1)\n    regularization = regularizer(L1_Weights) + regularizer(L2_Weights)\n\n    # loss\n    loss = cross_entropy_mean + regularization\n\n    # backward\n    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss)  # 基础梯度下降法\n    init = tf.global_variables_initializer()\n\n    with tf.Session() as sess:\n        sess.run(init)\n        for i in range(20000):\n            sess.run(train_step,feed_dict={xs:recon_imgs,ys:label})\n            if i % 50 == 0:\n                print(sess.run(loss,feed_dict={xs:recon_imgs,ys:label}))\n               \n        print(sess.run(out,feed_dict={xs:recon_imgs,ys:label}))</code></pre><p><br/></p><p>output：[[-0.09252657  0.29252994]，…… 全是一样的。。</p><p><br/></p><p>梯度下降试着用过动量法，但是不收敛。。</p></div>", "viewer": 4145, "tags": ["统计/机器学习", "计算机视觉", "Python", "人工神经网络", "TensorFlow"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>试试调一调learning rate吧，然后你只用一层的结构试试看，debug一下</p></div>", "lvl2_answer": ["好的！ 我先试试！ 谢谢！！"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>虽然还没找到错误出在哪，但是先记录下：</p><p>1. 把隐含层去掉后，即只有输入层和输出层，output能看的出来有分类，错误可能是出在了hidden_layer上，但是还没有找出来到底在哪。</p><p><br/></p><p>2. 正则化0.1太小了 可能找不到最优 我调成0.8之后就对了</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>有可能是l2_regularizer的参数太大了，Loss主要由regularization控制。因为你第一层的L1_weights个数36*36*150远大于L2_weights个数150*2，去掉L1后regularization急剧下降，LOSS由cross entropy控制。</p><p>你最好把两种配置的cross entropy和regularization都显示出来。</p></div>", "lvl2_answer": ["好的！ 谢谢~"]}]}