{"id": "1001580", "question": "激活函数ReLU和Sigmoid的优劣对比", "description": "<div class=\"col-md-11 col-xs-10\"><p>神经网络最常用的两个激活函数应该就是ReLU和Sigmoid函数。</p><p>它们各自有什么优劣？通常选哪个更好？</p><p><br/></p></div>", "viewer": 10594, "tags": ["统计/机器学习", "深度学习", "人工神经网络"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>一般而言在激活函数上使用的ReLu函数，理由如下：</p><p>第一，采用sigmoid等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯度时，求导涉及除法，计算量相对大，而采用Relu激活函数，整个过程的计算量节省很多。</p><p>第二，对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0，这种情况会造成信息丢失，从而无法完成深层网络的训练。</p><p>第三，Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。</p><p>综上所述建议使用ReLu函数。</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>1.Relu计算量小，</p><p>2.没有饱和区，没有梯度消失</p><p>3.用Relu去估计非线型函数时收敛更快。AlexNet论文里说大概比sigmoid快6倍。</p><p>4.有个缺点是Relu输出为0后，梯度没发反向传递，这个Relu就死掉了。在作参数（weight，bias）初始化时，不要把太多Relu弄死掉。所以后面有leakRelu之类。</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>对于二元分类问题，输出层的激活函数只能是sigmoid</p><p>但是中间隐藏层的确更推荐ReLU，原因也是正如另一个回答所说的三点</p><p>1. 计算量更小</p><p>2. 没有梯度消失的问题</p><p>3. 稀疏表达（奥卡姆剃刀原理）</p><p><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>应该叫 激励函数 吧</p></div>", "lvl2_answer": []}]}