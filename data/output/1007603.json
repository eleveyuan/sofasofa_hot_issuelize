{"id": "1007603", "question": "虚拟变量能否标准化？", "description": "<div class=\"col-md-11 col-xs-10\"><p>大多时候，处理数据时都是先对数值变量做标准化，再将分类变量one-hot编码。但有时出现这种情况：<br/>对于某些连续特征，我需要将其二元化（如：大于0时，值指派为1；等于0时，值指派为0）。做了二元化后，无需one-hot编码，但是在标准化时，因为其属于数值类型，也会被一并标准化。那么，这样的标准化是否可行？在标准化时，我是否应该把这部分值为0/1的二元离散变量排除在外？</p></div>", "viewer": 2742, "tags": ["统计/机器学习", "回归分析", "数据预处理", "Python"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>我个人的习惯是把已经二元化的变量跳过，只对非二元的数值特征进行标准化。</p><p>如果你有时间富余，可以两者方式都试试，交叉验证一下，看哪个效果好。我的经验是通常两种方法效果差不多。所以本质上没差别。</p></div>", "lvl2_answer": ["感谢您的回复！刚刚试了下，二元数值标准化后，性能略微下降。不过可能主要是因为试的几个是稀疏特征~总体效果确实相差不大~"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>以神经网络为例说明为什么要标准化输入输出。假如输入输出variance比例为1000，有10层网络，那每层网络就需要缩小信号的variance2倍。但是我们一般把网络参数初始化为N(0,1),反而会增加variance。(假设xy独立，var(x+y)=var(x)+var(y) =var(x)+1)。此时初始化参数和收敛参数相差太远，会增加收敛时间。如果标准化会，初始化参数会里收敛值近一点。最好是用He Kaiming initialization, 这样收敛会更快。</p><p>0,1的onehot编码，variance离1差距不大，嫌麻烦可以不做标准化。如果输入variance比1大很多，需要标准化。</p></div>", "lvl2_answer": ["请问二元(0/1)变量，variance不是总小于1吗，您说的“variance比1大很多”应该怎么理解？感谢您的回复！", "你看错了，我是说差距不大，不是比1大。"]}]}