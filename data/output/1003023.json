{"id": "1003023", "question": "为什么说非平衡（倾斜）的数据不好？", "description": "<div class=\"col-md-11 col-xs-10\"><p>在面经里经常看到考官问数据非平衡、倾斜、正负比例相差很大，该怎么办，需要哪些预处理。</p><p>但是我还不是很清楚为什么非平衡的数据不好？它给模型训练带来了哪些麻烦呢？</p></div>", "viewer": 4108, "tags": ["统计/机器学习", "监督式学习", "数据预处理", "开放问题"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>举个简单的例子，如果数据集中正例远多于负例（比如说99:1），那么我们的模型即使什么都不学习，只要预测数据为正例，就可以得到很高的准确率（比如说99%），但是这并不是一个好模型，因为它无法识别出负例，这种极端的情况下，我们通常直观上的一些评价标准并不适用这种模型。需要引用F1-Score这样的评价方法。而且在模型的训练过程中，比如说卷积神经网络，过多正例使其Loss在整个Loss中占主导地位，从而容易忽略了负例的Loss，很难收敛到一个很好的结果，通常要在Loss中加入权重，比如说Focal Loss的方法。</p></div>", "lvl2_answer": ["谢谢！如果对于逻辑回归（它用的logloss），非平衡数据对模型有影响吗？", "客气客气，我觉得有影响，正例远多于负例的情况下，对loss函数最小化的优化方向很有可能是overfitting训练集的正例，因为这样才能最有效地最小化loss函数。即使模型学习到了负例的特征从而使loss中负例部分减小，但是其所占比例太低，对于最终loss函数的值影响太小。这只是个人的理解，我也在入门学习中，应该还有很多纰漏。"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px;\">机器学算法在不平衡数据上表现往往会下降</span></p><p><span style=\"font-size: 14px;\">原因有：</span></p><p><span style=\"font-size: 14px;\">1. 目标变量的分布不均匀使得算法精度下降，对于小类的预测精度会很低，因为学习样本不够多</span></p><p><span style=\"font-size: 14px;\">2. 如果算法本身是精度驱动的，即该模型的目标是最小化总体误差，而小类对于总体误差的贡献很低</span></p><p><span style=\"font-size: 14px;\">3. 有些</span><span style=\"font-size: 14px;\">算法本身的前提假设就是数据集的类分布均衡，同时它们也可能假定不同类别的误差带来相同的损失</span></p></div>", "lvl2_answer": []}]}