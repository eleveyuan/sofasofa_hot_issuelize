{"id": "1000303", "question": "随机森林会发生过拟合（overfitting）吗？", "description": "<div class=\"col-md-11 col-xs-10\"><p>Random Forest会有过拟合的情况发生吗？</p></div>", "viewer": 18193, "tags": ["统计/机器学习", "监督式学习", "开放问题", "随机森林"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>相对于单个的Decision Tree，Random Forest不太容易over-fitting。Over-fitting的主要原因是因为模型学习了太多样本中的随机误差。因为Random Forest随机选择了样本和特征，并且将很多这样的随机树进行了平均，这些随机误差也随之被平均，乃至相互抵消了。但是这不代表它不会</p><p>很多人说Random Forest不会over-fitting。相信很多人也亲身经历过，我自己也见识过过RandomForest over-fitting了。</p><p>防止RandomFroest过拟合，一个方法是控制每个树的深度，深的树有可能会过拟合；另外一个是对模型进行交叉验证。</p></div>", "lvl2_answer": ["Feature选少点，增加随机性，也能防止overfitting", "谢谢！"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>任何机器学习算法都无法彻底避免过拟合的。</p><p>这是由机器学习的本质决定的，不是在技术层面可以修复的。 机器学习的核心在于泛化，但是数据又不可避免的包含noise，所以泛化就无法避免多多少少地也把noise包含进去。对于一个算法，它是没有办法区分noise和好数据的。因为这些通常是人也无法知道的，我们说一个toy dataset中某些数据是noise，这是我们的prior，机器是没有办法知道的。</p><p>所以随机森林当然也会过拟合，即使Breiman确实说过它不会overfitting。他口中的不会过拟合不过是指当树的数量够大多时，随着training error的持续下降，test error会稳定，而不是像很多算法一样，开始快速上升。有过手头经验的都知道，training error和test error之间始终是有差距的，他们的差值也通常可以看作是过拟合导致的。这个差值越大可以看作过拟合越厉害。</p></div>", "lvl2_answer": []}]}