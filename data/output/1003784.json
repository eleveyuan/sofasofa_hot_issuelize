{"id": "1003784", "question": "激活函数RELU在0点的导数是多少？", "description": "<div class=\"col-md-11 col-xs-10\"><p>RELU(x) = max(x, 0)所以它是算分段的，小于0的时候导数是0，大于0的时候导数是1，在0点不可导。</p><p>那实际操作或者计算的时候RELU在0点的导数我们认为是0还是1呢？</p></div>", "viewer": 13471, "tags": ["统计/机器学习", "人工神经网络"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>relu在0点的确不可导，正如@huluobo说的，可以用$\\log(1+e^x)$来近似，这个函数是连续的，它在0点的导数是0.5。也就是相当于relu在0点的导数取为0.5，也正好是0和1的均值。</p><p>也可以就规定relu在0点的取值是0或者1，这样也比较方便。tensorflow里面就默认relu在0点的导数是0。</p></div>", "lvl2_answer": ["谢谢"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>用$\\ln(1 + e^x)$来approximate吧</p></div>", "lvl2_answer": ["谢谢"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>因为数值计算中有roundoff error，计算机里的浮点数0表示是一个0周围以roundoff error为宽度的邻域，并不是一个点。所以tensorflow里Relu在0（计算机中的点，数学上的邻域）的导数应该是0和1两条不连续的线段。为简便，默认只取0。</p></div>", "lvl2_answer": []}]}