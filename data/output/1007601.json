{"id": "1007601", "question": "pyspark返回每个分组某个值最大的行", "description": "<div class=\"col-md-11 col-xs-10\"><p>我现在一个hive表里有type，value，id等等好几列，我想返回type每个分组里value最大的行。</p><p>不能用groupby max，因为这样只能返回每个分组的最大值，而不是完整的行。我用pyspark应该怎么做呢？</p></div>", "viewer": 3215, "tags": ["统计/机器学习", "Python", "mysql"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>这种用reducebykey就可以了</p><pre><code class=\"python\">from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\ndf = sql_context.createDataFrame([\n(\"A\", \"ARON\", 1),\n(\"A\", \"BILL\", 2),\n(\"A\", \"CLAIR\", 3),\n(\"B\", \"DANIEL\", 1),\n(\"B\", \"ERIC\", 4),\n(\"B\", \"FRANK\", 2),\n], [\"id\", \"name\", \"weight\"])\n\ndef max_row(row1,row2):</code></pre><pre><code class=\"python\">    if row1['weight'] &gt; row2['weight']:\n       return row1\n    else:\n        return row2\n\n\ndf_rdd = df.rdd.map(lambda row: (row['id'], row)).\\\nreduceByKey(lambda row1,row2: max_row(row1,row2)).map(lambda row: row[1])\nsql_context.createDataFrame(df_rdd).show()</code></pre><p><br/></p><p>或者用分组排序后取第一个</p><pre><code class=\"python\">\nwindows_spec = Window.partitionBy(\"id\").orderBy(F.col(\"weight\").desc())\nmax_df = df.withColumn(\"rank\", F.rank().over(windows_spec)).filter(\"rank =1\")\nmax_df.show()\n</code></pre><p> 可能第一个会快一些吧，一般也都是用第一个</p></div>", "lvl2_answer": ["谢谢"]}]}