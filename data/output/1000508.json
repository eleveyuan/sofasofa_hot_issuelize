{"id": "1000508", "question": "logloss的取值范围是多少？一般好的分类器能达到多少？", "description": "<div class=\"col-md-11 col-xs-10\"><p>我在做一个二元分类器，我现在的logloss是1.1左右。</p><p>logloss是可以大于1的吗？它的正常范围应该是多少？一般多小的logloss算是好的分类器？</p></div>", "viewer": 27337, "tags": ["统计/机器学习", "监督式学习", "损失函数"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px;\">$logloss$可以大于1，但是对于二元分类来说，大于1说明这个模型是比较糟糕。</span></p><p><span style=\"font-size: 14px;\">回顾一下$logloss$的公式</span></p><p><span style=\"font-size: 14px;\">$$logloss=−\\sum_{i=1}^n\\left(\\frac{y_i}{n}\\text{log}(p_i)+\\frac{1−y_i}{n}\\text{log}(1−p_i)\\right)$$</span></p><p><span style=\"font-size: 14px;\">其中$n$是测试样本的个数，$p_i$为预测第$i$个样本为阳性的概率，$y_i$表示第$i$个预测样本的真实标签（1代表阳性，-1代表阴性）。</span></p><p><br/></p><p><span style=\"font-size: 14px;\">我一开始就说了大于1的logloss是很差的。我在</span><span style=\"font-size: 14px;\">Gakki下面留言说0.8依然很差。我凭什么这么说？</span></p><p><span style=\"font-size: 14px;\">假如我们现在有个训练集，100万个数据点，其中10万个为阳性，那么总体上每个样本为1的概率可近似认为是0.1。通常来说，测试集的分布是非常接近于训练集的，那么测试集中大概有10%的样本为阳性。如果我们预测测试集中每个样本为1的概率都为0.1，那么logloss会是多少呢？</span></p><p><span style=\"font-size: 14px;\">$$logloss=−\\left(0.1\\text{log}(0.1)+0.9\\text{log}(1−0.1)\\right)\\approx 0.325$$</span></p><p><span style=\"font-size: 14px;\">假如总体分布是每个样本以$p$的概率为阳性，我们预测每个样本为阳性的概率都为$p$，也就是$p_i=p$，那么$logloss$是多少呢？</span></p><p><span style=\"font-size: 14px;\">很显然</span></p><p><span style=\"font-size: 14px;\">$$logloss=-p\\log (p) - (1-p) \\log(1-p)$$</span></p><p><span style=\"font-size: 14px;\">若$p=0.1$，$logloss = 0.325$</span></p><p><span style=\"font-size: 14px;\">若$p=0.2$，$logloss = 0.500$</span></p><p><span style=\"font-size: 14px;\">若$p=0.3$，$logloss = 0.611$</span></p><p><span style=\"font-size: 14px;\">若$p=0.4$，$logloss = 0.673$\n</span></p><p><span style=\"font-size: 14px;\">若$p=0.5$，$logloss = 0.693$</span></p><p><span style=\"font-size: 14px;\">若$p=0.6$，$logloss = 0.673$\n</span></p><p><span style=\"font-size: 14px;\">若$p=0.7$，$logloss = 0.611$</span></p><p><span style=\"font-size: 14px;\">若$p=0.8$，$logloss = 0.500$</span></p><p><span style=\"font-size: 14px;\">若$p=0.9$，$logloss = 0.325$</span></p><p><span style=\"font-size: 14px;\">所以最差的情况就是，样本正好是一半阳性一半阴性，此时你按照上面方面预测（乱猜）出的logloss是0.693。</span></p><p><span style=\"font-size: 14px;\">换句话说，只要loglss是在0.693以上，就基本说明了模型是失败的。</span></p><p><br/></p></div>", "lvl2_answer": ["厉害厉害", "哈哈哈，清楚了"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>先搞清楚logloss的定义</p><p>$$\\text{logloss}=-\\frac{1}{n}\\sum_{i=1}^n y_i\\log(p_i)+(1-y_i)\\log(1-p_i),$$</p><p>其中$n$是测试数据的个数，$y_i$是第$i$条记录的真实值（0或者1），$p_i$是你预测的第$i$条记录是1的概率。</p><p>很明显$\\text{logloss}$的取值范围是$[0,+\\infty)$。试想一下，有个记录的真实是1，但是你预测它是1的概率为0，$-\\log 0$就是正无穷，你的$\\text{logloss}$就是正无穷大。</p><p>所以你得到1.1是可能发生的。</p><p>LogLoss是越小越好，至于多小是好，并没有统一标准。这个要根据实际问题来看。我参加过两个用LogLoss做标准的比赛，一个比赛的第一名是0.01左右，另外一个是0.4左右。用其他数据的LogLoss作为自己模型的标准，参考意义不大。</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>可以参考这个（<a href=\"http://sofasofa.io/leaderboard.php?id=5\" target=\"_blank\">机器读中文2：“辨古识今”-排行榜</a>）咯</p><p>感觉0.2到1之间吧</p><p><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px;\">﻿</span><span style=\"font-size: 14px;\">cross entropy 的定义</span></p><p><span style=\"font-size: 14px;\">$$H(p,q)=-\\sum_{y=(0,1)}p(y)\\log q(\\hat{y}=y)$$</span></p><p><span style=\"font-size: 14px;\">$$=H(p)+D_{KL}(p||q)$$</span></p><p><span style=\"font-size: 14px;\">$$=-\\sum_{y=(0,1)}p(y)\\log p(y)+D_{KL}(p||q)$$</span></p><p><span style=\"font-size: 14px;\">其中$y$是真实输出标签，$\\hat{y}$是预测输出标签，</span><span style=\"font-size: 14px;\">$p(y)$是$y$的概率密度函数（pdf）,$q(\\hat{y})$是预测的$\\hat{y}$的pdf。cross entropy分两部分：</span></p><p><span style=\"font-size: 14px;\">1. $H(p)$是真实标签$y$的熵，表示其$y$的分散程度，对于训练数据是一个<span style=\"color: rgb(255, 0, 0);\">常数</span>，只和数据有关，和预测模型无关。</span></p><p><span style=\"font-size: 14px;\">2.$D_{KL}(p||q)$是 Kullback–Leibler divergence 表示真实和预测的pdf之间的匹配程度。这才能衡量预测模型是否好。</span>$D_{KL}(p||q)=0$表示$p$和$q$完全匹配。</p><p><br/></p><p><span style=\"font-size: 14px;\">需要注意$H(p_i,q_i)$只是第i个数据点的cross entropy。我们求的是所有训练数据点cross entropy的和。</span></p><p><span style=\"font-size: 14px;\">$$J(w)=\\frac{1}{N}\\sum_{i=1}^{N}H(p_i,q_i)$$</span></p><p><span style=\"font-size: 14px;\">$$=-\\frac{1}{N}\\sum_{i=1}^{N}(y_i\\log (\\hat{y_i})+(1-y_i)\\log (1-\\hat{y_i})))$$</span></p><p><span style=\"font-size: 14px;\">如果是logistic regression, $\\hat{y_i}=g(w^Tx_i)=1/(1+e^{-w^Tx_i})$。</span></p><p>相似我们可以得到所有训练数据的entropy和</p><p>$$H(y)=-\\frac{1}{N}\\sum_{i=1}^{N}(y_i\\log (y_i)+(1-y_i)\\log (1-y_i)))$$</p><p>而$D_{KL}(w)=J(w)-H(y)$才是表示预测模型好坏的指标。比如平衡过的数据，$y_i=0.5$，则$H(y)=-\\frac{1}{N}*N*2*0.5\\log 0.5=-log 0.5=0.693$，我们得到的logloss需要$-0.693$。</p><p>结论：</p><p>1.如果数据相同，可以直接比logloss。</p><p>2.如果数据不同，需要减去$H(y)$才能比较$D_{KL}(w)$。</p><p>3.或者比较normalized cross entropy$\\dfrac{J(w)}{H(y)}$</p><p><br/></p><p><span style=\"font-size: 14px;\"><br/></span></p><p><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>我之前有过0.8的loglss，仅供参考</p></div>", "lvl2_answer": ["对于二元分类来说，0.8的logloss算是失败了吧", "我同意，0.8的确算是非常差的。", "这么一看的确是的，0.8应该挺糟糕的"]}]}