{"id": "1002578", "question": "决策树或者随机森林能够直接处理missing data吗？", "description": "<div class=\"col-md-11 col-xs-10\"><p>决策树或者随机森林能够直接处理missing data吗？还是需要做预处理？</p></div>", "viewer": 8583, "tags": ["统计/机器学习", "监督式学习", "数据预处理"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px;\">一。决策树/随机森林（RF）直接处理missing data的方法：</span></p><p><span style=\"font-size: 14px;\">1.CART中可用surrogate splits，但是根据</span><a href=\"https://arxiv.org/pdf/0811.1645.pdf\" target=\"_blank\"><span style=\"font-size: 14px;\">Random Survival Forests</span></a><span style=\"font-size: 14px;\">，RF不推荐surrogate splits。</span></p><p><span style=\"background-color: rgb(239, 239, 239);\"><span style=\"font-size: 14px;\">\"Although surrogate splitting works well for trees, the method may not </span>be well suited for forests. Speed is one issue. Finding a surrogate split is computationally intensive and may become infeasible when growing a large number of trees, especially for fully saturated trees used by forests. Further, surrogate splits may not even be meaningful in a forest paradigm. RF randomly selects variables when splitting a node and, as such, variables within a node may be uncorrelated, and a reasonable surrogate split may not exist. Another concern is that surrogate splitting alters the interpretation of a variable, which affects measures such as VIMP.\"</span></p><p><span style=\"font-size: 14px;\">2.用C4.5代替CART，C4.5计算information gain时没直接用到missing data。</span></p><p><span style=\"font-size: 14px;\">二。或者用填值（imputation）的方法预处理：</span></p><p><span style=\"font-size: 1rem;\"><span style=\"font-size: 14px;\">1.用average/median/mode填；或根据</span><a href=\"https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#missing1\" target=\"_blank\"><span style=\"font-size: 14px;\">原始的RF</span></a><span style=\"font-size: 14px;\">，用加权后的average/median/mode填，权重是missing data point和其他data point的相识度。</span></span></p><p><span style=\"font-size: 1rem;\"><span style=\"font-size: 14px;\">2.用复杂算法去估计missing data，比如R中</span><a href=\"https://www.rdocumentation.org/packages/SpatioTemporal/versions/1.1.7/topics/SVDmiss\" target=\"_blank\"><span style=\"font-size: 14px;\">SVDmiss</span></a><span style=\"font-size: 14px;\">，交替地算SVD和填值。还有</span></span><a href=\"https://github.com/mayer79/missRanger\" target=\"_blank\"><span style=\"font-size: 14px;\">missRanger</span></a><span style=\"font-size: 14px;\">和</span><span style=\"font-size: 14px;\"><a href=\"https://www.rdocumentation.org/packages/missForest/versions/1.4\" target=\"_blank\">missForest</a>，交替的填值和进行随机森林。</span></p><p><span style=\"font-size: 1rem;\"><span style=\"font-size: 14px;\">3.</span><a href=\"https://stats.stackexchange.com/questions/98953/why-doesnt-random-forest-handle-missing-values-in-predictors\" target=\"_blank\"><span style=\"font-size: 14px;\">这里</span></a><span style=\"font-size: 14px;\">提到\"on the fly imputation\" (OTFI)，随机地填其他数据点中出现过的值，但是填充的值不用于split的计算。</span></span></p><p><span style=\"font-size: 14px;\">4.</span><a href=\"http://www.staff.science.uu.nl/~feeld101/pkdd99.pdf\" target=\"_blank\"><span style=\"font-size: 14px;\">Handling missing data in trees: surrogate splits or statistical imputation ?</span></a><span style=\"font-size: 14px;\">其中说填值方法计算量小，效果好。</span></p><p><span style=\"font-size: 14px;\">我的理解是最好用填值，因为填值和训练是独立的两个步骤，填值后数据比较稳定，利于分析，且训练计算量小。对于</span><span style=\"font-size: 14px;\">决策树可以不用填值，但是随机森林需要填值。</span></p><p><br/></p></div>", "lvl2_answer": ["谢谢解答！我还有一个问题，以C4.5为例，missing data不影响information gain的计算，可以得到数值特征的splitting阈值，那么在这个节点上split样本的时候，missing data是放在&lt;=的左边呢，还是&gt;的右边呢？", "1.分到data point多的一边\n2.一个data在两边都有一部分，按两边data point数比例。比如nonmissing data在左面有90个，在右面10个，那么一个data90%分左面，10%分右面。\n3.随机分在两边，分配概率正比与两边data point数。", "懂了，谢谢大神！"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>python里sklearn不能，pycaret能。</p><p>原理上说随机森林是支持有缺失值的。</p></div>", "lvl2_answer": []}]}