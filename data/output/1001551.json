{"id": "1001551", "question": "kNN算法有哪些缺点？", "description": "<div class=\"col-md-11 col-xs-10\"><p>kNN算是比较早期的算法了，但是业界似乎用的并不多，kNN有哪些明显的缺点或者短板吗</p><p>欢迎开放讨论</p><p><br/></p></div>", "viewer": 9553, "tags": ["统计/机器学习", "监督式学习", "开放问题"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>KNN属于懒惰算法，对测试样本分类时的计算量大，内存开销大，评分慢\r\n</p><p>可解释性较差，无法给出决策树那样的规则。\r\n</p><p>kNN算法必须保存全部的数据集，如果训练数据集很大，那么就需要耗费大量的存储空间。此外，由于必须对待分类数据计算与每一个训练数据的距离，非常耗时。</p><p><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>1. 计算代价很大</p><p>2. 无法处理categorical变量</p><p>3. 对变量的缩放非常敏感</p><p>4. 难以处理不同单位和不同数值范围的变量</p><p>5. 对高维数据表现不佳</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>kNN还有个问题就是k的选择。k总是在过拟合与欠拟合之间游走。</p><p>你可以说通过cross validation来选k，可是呢，你选出的k是全局适用的，而每个局部都是这个k最佳。</p><p>所以kNN总是无法避免地在一些区域过拟合，同时在另一些区域欠拟合。</p><p><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>不能调参算缺点么？</p><p>比较牛的模型，一般都有很多参数可以调，比如神经网络或者xgboost。</p><p>但是对于K近邻模型来说，本来水平就一般，连参数也只有一个k可以调，就显得有点弱上加弱了。</p><p><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>k-NN怕高维啊怕高维啊怕高维，说三遍</p><p>特别是遇到没什么用的特征，k-NN会不知所措，而且被那些没有什么prediction power的特征牵着鼻子走。</p><p>所以k-NN必须要先做特征选择，不然这些不相关的特征会影响到分类效果，因为k-NN自己无法学习到哪些特征重要、哪些不重要</p><p><br/></p></div>", "lvl2_answer": ["对，高维和不相干特征的影响，这个是要害。"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>如果用K近邻模型做回归的话，一个比较明显的缺陷，就是K-NN无法做out of sample的回归预测。</p><p>因为用K-NN做回归的时候，预测值是它附近几个值的平均值。所以预测值不可能超过预测样本的最大值，也不可能小于样本的最小值。这个缺点其实是和回归树一样的。</p><p><br/></p></div>", "lvl2_answer": []}]}