{"id": "1000661", "question": "如何检验两个样本是同分布的？", "description": "<div class=\"col-md-11 col-xs-10\"><p>假如我有两个样本，有没有什么假设检验的方法可以判断这两个样本是否服从同一个概率分布？</p></div>", "viewer": 18439, "tags": ["统计/机器学习", "假设检验", "概率分布"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>如果是非数值的样本(multinomial)，那么可以用卡方检验。</p><p>如果是数值样本，可以用柯尔莫哥洛夫-斯摩洛夫检验(K-S test)。</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>Categorical的数据：用卡方检验</p><p>Numerical的数据：用KS</p><p>Binary的数据：用T test</p></div>", "lvl2_answer": ["Binary的数据也可以用$\\chi^2$"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>提一个我目前正在看而且比较冷门的吧，Wasserstein distance. </p><p>我觉得挺有帮助的参考文章有 <a href=\"https://vincentherrmann.github.io/blog/wasserstein/\">Wasserstein GAN and the Kantorovich-Rubinstein Duality</a> 还有 stackExchange上面这个问题<a href=\"https://stats.stackexchange.com/questions/280356/kullback-leibler-distance-for-comparing-two-distribution-from-sample-points\">Kullback-Leibler distance for comparing two distribution from sample points</a> (原本以为KL divergence也可以的但是看了这个之后就有些犹豫了就不说出来了hhh)</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>说一个不用假设检验的想法，对于数值的概率分布，把两个分布的累积分布函数的曲线画出来，然后求两个曲线的距离（L1或者L2）。人为设置一个阈值，小于它就说明它们两条曲线够接近，分布够相似。</p></div>", "lvl2_answer": []}]}