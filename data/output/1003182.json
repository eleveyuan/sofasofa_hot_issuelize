{"id": "1003182", "question": "最小二乘法与最小二乘支持向量回归的优劣", "description": "<div class=\"col-md-11 col-xs-10\"><p><span style=\"font-size: 18px;\"><span style=\"font-size: 14px;\">以一个一维的的回归为例子，使用</span><b><span style=\"font-size: 14px;\">二阶最小二乘回归</span></b><span style=\"font-size: 14px;\">，最终需要得到公式: $y=aX^2+bX+C$</span></span><span style=\"font-size: 14px;\">，在最小二乘的计算方法下，要通过已知点拟合得到函数关系，需要做的计算是求解一个三个未知数，三个方程的方程组，</span><b><span style=\"font-size: 18px;\"><span style=\"font-size: 14px;\">此时其实不用管数据</span><i><span style=\"font-size: 14px;\">N</span></i><span style=\"font-size: 14px;\">量有多少，数据量的大小体现在那三个方程的各项相加当中</span></span></b><span style=\"font-size: 14px;\">。</span></p><p><span style=\"font-size: 18px;\"><span style=\"font-size: 14px;\">而如果要使用</span><b><span style=\"font-size: 14px;\">最小二乘支持向量回归</span></b><span style=\"font-size: 14px;\">的方法，</span><b><span style=\"font-size: 14px;\">最终求解的方程组是一个有N+1个未知数，N+1个方程组成的方程组</span></b><span style=\"font-size: 14px;\">，一般俩说，数量都是巨大的，其实最小二乘回归也可以体现出每个数据的作用，那么在这个意义上，</span><b><span style=\"font-size: 14px;\">是不是说，最小二乘法回归会比最小二乘支持向量回归更加高效而简洁呢</span></b><span style=\"font-size: 14px;\">？而且更进一步地，最小二乘回归不存在核函数的问题，解释性会更好一些。</span></span></p></div>", "viewer": 8296, "tags": ["统计/机器学习", "回归分析", "开放问题"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px;\">这是两种解线性模型$X^Tw=y$的回归方法，其中$X$是 $d$x$n$的$d$维$n$个列向量输入数据，$w$是$d$x$1$参数，$y$是输出。最小二乘LS是以随机变量（$X$中的行）为计算相关性的单位；最小二乘支持向量回归LS_SVR是以数据点（$X$中的列）为计算相似性的单位，类似kNN，以数据点间的距离的估计为基础。回归都是求条件期望（conditional mean），LS是以输入随机变量为条件，并学习输入随机变量到输出随机变量的模型；LS_SVR是以测试点离训练点的距离矩阵为条件,并学习这个距离矩阵到输出的映射。</span></p><p><span style=\"font-size: 14px;\">1.公式：</span></p><p><span style=\"font-size: 14px;\">LS：$\\hat{y}=X_{new}^Tw=X_{new}^T(XX^T)^{-1}XY$, </span><span style=\"font-size: 14px;\">这里$\\text{Cov}(X,X)=XX^T$</span><span style=\"font-size: 14px;\">衡量的是输入随机变量的相关性，$\\text{Cov}(X,Y)=XY^T$</span><span style=\"font-size: 14px;\">衡量的是输入与输出随机变量的相关性。</span></p><p><span style=\"font-size: 14px;\">LS_SVR： $\\hat{y}=X_{new}^Tw=X_{new}^TX\\alpha=X_{new}^TX(X^TX+1/\\gamma I)^{-1}Y$, </span><span style=\"font-size: 14px;\">其中$w=X\\alpha$认为参数$w$是输出数据$X$的加权平均，$\\alpha$是每个点的重要性，$\\alpha$大的点为support vector。</span><span style=\"font-size: 14px;\">应用kernel后变为$\\hat{y}=K(X_{new},X)(\\Omega +1/\\gamma I)^{-1}Y$</span>。$K(X_{new},X)$ 衡量测试与训练数据点的距离，$\\Omega $是训练数据点的距离矩阵，$1/\\gamma I$是正则项，让$\\Omega +1/\\gamma I$可逆。</p><p><span style=\"font-size: 14px;\">2.优缺点：</span></p><p><span style=\"font-size: 14px;\">LS优点是在$n&gt;d$时，$(XX^T)^{-1}$计算量要远小于$(\\Omega +1/\\gamma I)^{-1}$；可直接得到$w$，解释性好；在测试点周围没有训练点时，做extrapolation。</span></p><p>LS缺点是要求所有训练数据点的随机变量要服从相同分布，而且要求测试和训练的随机变量要同分布。也就是说数据要分布一致，不能是mixture之类的数据。所以LS只能处理简单的数据。LS也不容易过拟合。</p><p>LS_SVR优点是可以用kernel代替计算高维上的距离；可处理mixture类的复杂数据。</p><p>LS_SVR缺点是训练计算量大；距离矩阵不稀疏；要求测试点周围有训练点，只能做interpolation；当使用kernel后不能显性求$w$,可解释性差；较容易过拟合（比如用RBF且半径设的很小）。<br/></p><p><span style=\"font-size: 14px;\">可看到除了计算量外，它们优缺点是互补的。如果问题简单，测试和训练数据的随机变量分布一致，用LS。如果测试点周围能找到很多训练点，可用LS_SVR。具体用哪个，主要看对模型估计有信心，还是对距离矩阵向输出的映射的估计有信心。</span></p><p>-----------------------------------</p><p>补充一下， 我觉得LS_SVR也有很好的解释性。LS中$w$是输入变量到输出变量的线性映射关系。类同，LS_SVR$\\hat{y}=K(X_{new},X)\\alpha$，其中$\\alpha$是把一个$n$维距离向量到1维输出的线性映射关系。再次反映了LS是基于随机变量，LS_SVR是基于距离。</p></div>", "lvl2_answer": ["十分感谢", "我看过一些书和文献之后也是这样的感觉，LSSVR所谓解释性差都只是集中在“核函数”这一个点而已，其数学逻辑是很清晰的"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>这几天看了些资料，特别是[Saunders C, Gammerman A, Vovk V. Ridge Regression Learning Algorithm in Dual Variables[C]// International Conference on Machine Learning, Madison, Wi, July. 1998:515-521.]这个文献。其实LSSVR就是一种岭回归的Kernel化，即正则化的最小二乘回归的Kernel化，其求解步骤较最小二乘而言是采用有约束条件下的Lagrange乘数法，其实最小二乘直接使误差函数做偏导的缘由是可以看做一个开口向上的二次函数，偏导为0则在谷点，用Lagrange法去求解最小二乘问题结果也是一样的。于是LSSVR和最小二乘法由此便同意起来了，当正则化参数为零（即表征高维空间中平面距离的项——惩罚项为零）且核函数就取X<sup>T</sup>X时，LSSVR退化成为一个最小二乘回归的形式。</p><p>关于LSSVR的优势在于，第一是通过正则化的运用，避免了最小二乘回归中稀疏矩阵A为奇异矩阵的情况；第二是通过Kernel化，将问题在一个对数据点而言的高维线性可拟合空间中进行拟合操作，这样可以比较方便地获得一个好的结果，而至于计算量，其实当采用Lagrange乘数法来对最小二乘问题进行求解时，也是需要求解一个和LSSVR等大的方程组。</p></div>", "lvl2_answer": ["正如你所说，加正则项让非可逆矩阵变可逆；kernel简化了高维的距离计算。但是这些都是次一级的数值计算的方法。LS和LSSVR最根本的区别是它们求逆的矩阵不同。LS对d×d的covariance matrix$XX^T$求逆，LSSVR对n×n的距离矩阵$X^TX$求逆。它们只是结构相似，但是并不统一"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>SVM的解释性并不好，线性回归完全没有这个问题</p><p>线性回归计算特性好，可以求矩阵，也可以在线小批量计算，非常灵活，现在都是大数据了，这点SVM比不了</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>我唯一能想到的LSVR一个相对优势就是，最小二乘法使用时必须先假定阶数，也就是说必须先验or猜测地知道回归函数的阶数，而采用LSVR则省去了这一限制，但是完全可以把最小二乘的阶数取高一些就ok了呀，那也还是相对于LSVR会更具有计算上的优势</p></div>", "lvl2_answer": []}]}