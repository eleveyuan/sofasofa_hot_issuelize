{"id": "1000884", "question": "PCA和SVD是一回事吗？", "description": "<div class=\"col-md-11 col-xs-10\"><p>大概我明白，是做矩阵分解、降维的。但是这两个概念我有点混淆，它们是一个概念吗？有区别吗？<span style=\"font-size: 1rem;\">多谢多谢！比心！</span></p><p><span style=\"font-size: 1rem;\"><br/></span></p></div>", "viewer": 8689, "tags": ["数学", "线性代数", "数值计算", "最优化"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>它们不是一个回事，但是本质上是一样的。PCA的理论基础是SVD。所以是可以看作是一回事。</p><p>假设数据集是矩阵$X$，$n$行$p$列。</p><p>简单来说，SVD可以把$X$分解成如下形式：</p><p>$$X=\\sum_{i=1}^r \\sigma_iu_iv_i^T$$</p><p>这里的$r$是矩阵$X$的秩，$\\sigma_i$是奇异值（为了方便，通常从$\\sigma_1$到$\\sigma_r$按照从大到小排列），$u_i$是高度为$n$的列向量，$v_i$是高度为$p$的列向量。<span style=\"font-weight: bold;\">$X$被表示为了$r$个成分的和。</span></p><p><br/></p><p>SVD的工作完成后，就轮到PCA出场了。PCA的中文是主成分分析。假设我们想将$X$降为到$k$维，我们就选择最大的$k$个奇异值和它对应的$u_i, v_i$，<span style=\"font-weight: bold;\">也就是前$k$个主要成分。</span>假设$\\sigma_i$按照从大到小排列，那么$X$降为到$k$维之后应该是这样的</p><p>$$\\sum_{i=1}^k\\sigma_iu_iv_i^T$$</p><p>上面这个矩阵就是$k\\times p$的矩阵。</p><p>如果想知道上面这个矩阵解释了原矩阵$X$方差的百分比，只需要计算</p><p>$$\\frac{\\sum_{i=1}^k \\sigma_i^2}{\\sum_{j=1}^r\\sigma_j^2}$$</p><p>通常来说，在用PCA降维时，我们保留90%或者95%的方差。也就是上面的分式至少要大于0.9。</p><p><br/></p></div>", "lvl2_answer": []}]}