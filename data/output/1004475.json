{"id": "1004475", "question": "随机森林的模型一般都很大么？", "description": "<div class=\"col-md-11 col-xs-10\"><p><span style=\"font-size: 1rem;\">就是10w条+特征数据</span><span style=\"font-size: 1rem;\">max_depth=30,n_estimators=500</span></p><p>跑出来了800M大小模型，<span style=\"font-size: 1rem;\">load都慢。。。</span></p></div>", "viewer": 5447, "tags": ["统计/机器学习", "监督式学习", "Python"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>你这个森林深度有点大啊，如果你把深度调小点，效果应该差不多。</p><p>另外你保存的时候，设置一下compress参数应该会减小很多空间</p><pre><code class=\"python\">from sklearn.externals import joblib\r\njoblib.dump(my_rf_model, \"rf.pkl\", compress=9)\r\n#my_rf_model是你训练好的模型</code></pre><p>compress的值是0到9之间，越大表示压缩程度越高。</p></div>", "lvl2_answer": ["您好，调整了一下测试 确实小了很多，参数为(max_depth=30,n_estimators=500, min_samples_leaf= 5)，10W条数据 compress=0时是300M+，compress=9时是50M+，这边数据量是有500W+如果按照这个比例的话其实压缩比是6倍左右，还是挺大的，请问下面的只能调整深度参数来解决了么？", "你的模型还是挺大的，最大深度为30，就意味着一棵树最多有2^29个分叉，2^30个叶节点。你的数据只有10万，应该是不需要这么大的深度。同时min_sample_leaf可能也要调整。有时候为了模型压缩牺牲一点精度是ok的。"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px;\">随机森林一般深度不要超过10，不然每棵树过拟合严重，而且就会有你遇到的问题，模型太大，不管是调用还是存储都是问题。</span></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>我觉得模型选择上挺好的，随机森林固有的两类随机性可以很好的处理特征较多的情况，但是深度有些大了，过拟合有很大的概率会出现，可读性也不会太好。</p><p><br/></p></div>", "lvl2_answer": []}]}