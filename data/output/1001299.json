{"id": "1001299", "question": "cross entropy是什么意思？", "description": "<div class=\"col-md-11 col-xs-10\"><p>cross entropy是什么意思？</p><p><br/></p></div>", "viewer": 4557, "tags": ["统计/机器学习", "监督式学习", "损失函数"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><b><span style=\"font-size: 14px;\">entropy是用于表示二元分类器的误差，</span><span style=\"font-size: 14px;\">而cross entropy则用于表示多元分类器的误差。</span></b></p><p><span style=\"font-size: 14px;\">对于一个用$n$的测试样本的$k$元分类器，那么cross entropy的计算公式如下：</span></p><p><span style=\"font-size: 14px;\">$$CrossEntropy=-\\frac{1}{n}\\sum_{i=1}^n \\sum_{j=1}^ky_{i,j}\\log p_{i,j}$$</span></p><p><span style=\"font-size: 14px;\">比如一个三元分类问题，有两个测试样本，第一个样本：</span></p><p><span style=\"font-size: 14px;\">预测出来的概率为$(0.5, 0.3, 0.2)$，实际标签为$2$，写成one-hot的形式是$(0,1,0)$；</span></p><p><span style=\"font-size: 14px;\">第二个样本：</span></p><p><span style=\"font-size: 14px;\">预测出来概率为$(0.6, 0.1, 0.3)$，实际标签为$1$，one-hot的形式为$(1, 0, 0)$</span></p><p><span style=\"font-size: 14px;\">那么cross-entropy为</span></p><p><span style=\"font-size: 14px;\">$$-\\frac{1}{2}(1\\times \\log(0.3)+ 1\\times\\log(0.6))$$</span></p><p><br/></p></div>", "lvl2_answer": []}]}