{"id": "1000352", "question": "逻辑回归的损失函数是怎么来的", "description": "<div class=\"col-md-11 col-xs-10\"><p>逻辑回归的损失函数是怎么来的？为什么定义成log-loss？</p><p>$$\\text{log-loss}=-\\frac{1}{n}\\sum_{i=1}^n\\left((y_i\\log p_i)+(1-y_i)\\log(1-p_i)\\right)$$</p></div>", "viewer": 13844, "tags": ["统计/机器学习", "回归分析", "监督式学习", "损失函数"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>Logistic regression的loss function不是凭空出现的，是根据逻辑回归本身式子中系数的最大似然估计推导而来的。</p><p>逻辑回归的式子是这样的</p><p>$$Pr(y_i = 1) = \\frac{1}{1+e^{-(\\beta_0+\\beta_1x_i)}}.$$</p><p>下面我们可以写出逻辑回归中的$\\beta_0$和$\\beta_1$的似然函数</p><p>\\begin{eqnarray*} L(\\beta_0,\\beta_1)&amp;=&amp;\\prod_{i=1}^n p(Y=y_i|X=x_i)\\\\ &amp;=&amp; \\prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}. \\end{eqnarray*}</p><p>其中$(x_i,y_i)$表示训练集中的每一个样本点。我们对似然函数取对数，可得</p><p>$$ \\log L(\\beta_0,\\beta_1) = \\sum_{i=1}^n\\left(y_i\\log(p(x_i))+(1-y_i)\\log(1-p(x_i))\\right).$$</p><p>我们希望上式越大越好，换句话说，对于给定样本数量$n$，我们希望$-\\frac{1}{n}\\log L(\\beta_0,\\beta_1)$越小越好，这个也就是LogLoss。</p><p>$$ \\text{log-loss} = - \\frac{1}{n}\\sum_{i=1}^n\\left(y_i\\log(p(x_i))+(1-y_i)\\log(1-p(x_i))\\right). $$</p><p>所以说逻辑回归的损失函数不是定义出来的，而是根据最大似然估计推导而来的。</p><p><br/></p></div>", "lvl2_answer": ["原来如此！和信息熵不谋而合！", "熵的log的底数是2，logloss的底数是e吧"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>逻辑回归是最大熵模型对应类别为两类时的特殊情况</p></div>", "lvl2_answer": []}]}