{"id": "1000903", "question": "用神经网络做二元分类，输出层用Sigmoid还是Softmax？", "description": "<div class=\"col-md-11 col-xs-10\"><p>我用神经网络做二元分类，在最后的输出层的地方，output的函数应该用什么？</p><p>用Sigmoid还是Softmax？哪个好？</p><p>谢谢！</p><p><br/></p></div>", "viewer": 8920, "tags": ["统计/机器学习", "监督式学习", "深度学习", "人工神经网络"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>本质上说它们是一个东西，所以没有所谓的谁好谁坏。</p><p>Softmax是广义的Sigmoid，因为Sigmoid只能做二元分类。Softmax是可以做多元分类。二元的Softmax其实就退化成了Sigmoid。</p><p><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px;\">对于二元分类Softmax和Sigmoid在数学上相似，但是在数值计算（numerical）上有点差别。</span></p><p><span style=\"font-size: 14px;\"> 对于Softmax，</span></p><p><span style=\"font-size: 14px;\">$P(y=1|X)=\\frac {e^{X^Tw_1}} {e^{X^Tw_0}+e^{X^Tw_1}}$，</span></p><p><span style=\"font-size: 14px;\">$X$是全连接层输入，$w$是全连接层参数，$X^Tw_k$是logits，全连接层输出。当$X^Tw_1=$inf，输出$P$</span><span style=\"font-size: 14px;\">=inf/inf=Nan；当$X^Tw_1=$-inf，$X^Tw_0=$-inf，输出$P$=0/0，会报错float division by zero。</span></p><p><span style=\"font-size: 14px;\">对于Sigmoid，</span></p><p><span style=\"font-size: 14px;\">$P(y=1|X)=\\frac {e^{X^Tw_1}} {e^{X^Tw_0}+e^{X^Tw_1}}$</span></p><p><span style=\"font-size: 14px;\">$=\\frac {e^{X^Tw_1}/e^{X^Tw_1}} {(e^{X^Tw_0}+e^{X^Tw_1})/e^{X^Tw_1}}$</span></p><p><span style=\"font-size: 14px;\">$=\\frac {1} {e^{X^T(w_0-w_1)}+1}$</span></p><p><span style=\"font-size: 14px;\">$=\\frac {1} {e^{X^T\\Delta w}+1}$</span></p><p><span style=\"font-size: 14px;\">当$X^T\\Delta w$为inf/-inf时，输出$P$为0/1。</span></p><p><span style=\"font-size: 14px;\">Softma需要学习两组$w_0,w_1$，而Sigmoid只需要学一组$\\Delta w$。所以最好用一位输出的Sigmoid。</span></p><p><br/></p><p><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>虽说它们似乎是等价的，不过感觉sigmoid更常用，更保险一点吧</p></div>", "lvl2_answer": []}]}