{"id": "1000442", "question": "除了PCA，还有什么降维的方法？", "description": "<div class=\"col-md-11 col-xs-10\"><p>才疏学浅，平时只用过PCA降维，大家还有什么其他的降维方法吗？</p></div>", "viewer": 12108, "tags": ["统计/机器学习", "数据降维", "特征选择", "开放问题"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-weight: bold;\">1. High Correlation</span></p><p>如果两个feature的correlation大于某个阈值（自己设定的，比如0.3），就删掉其中一个。</p><p><span style=\"font-weight: bold;\">2. Low variance</span></p><p>如果一个feature的数据方差小于某个阈值（自己设定），就把它删掉。</p><p><span style=\"font-weight: bold;\">3. Missing</span></p><p>如果这一列有很多missing，就把它删掉。</p><p><span style=\"font-weight: bold;\">4. Random Forests</span></p><p>Random Forests训练之后，可以返回所有特征的重要性，我们可以选择重要性最高的一部分特征，比如20%。</p><p><span style=\"font-weight: bold;\">5. Stepwise selection</span></p><p>逐步选择特征，可以向前选择，也可以向后消去。</p><p><span style=\"font-weight: bold;\">6. Random Projection</span></p><p>类似于PCA，但是这个投影是随机的，而非像PCA那样是正交的。</p><p><b>7. AutoEncoder</b></p><p>类似于word2vec，我们只提取神经网络的中间层的结果作为降维的结果，当激活函数都是线性时，效果与PCA相似。</p></div>", "lvl2_answer": ["有具体介绍Random Projection的资料吗？"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>如果你有response的话，还可以用partial least square. <a href=\"https://en.wikipedia.org/wiki/Partial_least_squares_regression\" target=\"_blank\">https://en.wikipedia.org/wiki/Partial_least_squares_regression</a></p><p>然后其他的降维算法可以参考scikit-learn的roadmap : <a href=\"http://scikit-learn.org/stable/tutorial/machine_learning_map/\" target=\"_blank\">http://scikit-learn.org/stable/tutorial/machine_learning_map/</a></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>还有t-SNE</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px;\">补充一下，还有</span><a font-size:=\"\" font-style:=\"\" font-variant-caps:=\"\" font-variant-ligatures:=\"\" font-weight:=\"\" href=\"http://sofasofa.24xi.org/forum_main_post.php?postid=1000433\" letter-spacing:=\"\" microsoft=\"\" none=\"\" normal=\"\" orphans:=\"\" sans-serif=\"\" start=\"\" style=\"box-sizing: inherit; background-color: rgb(255, 255, 255); color: rgb(2, 117, 216); text-decoration: none; font-family: Arial, \" target=\"_blank\" text-align:=\"\" text-indent:=\"\" text-transform:=\"\" white-space:=\"\" widows:=\"\" word-spacing:=\"\" yahei=\"\"><span style=\"font-size: 14px;\">feature hashing</span></a><a href=\"http://sofasofa.24xi.org/forum_main_post.php?postid=1000433\" target=\"_blank\"></a></p></div>", "lvl2_answer": ["这个只适用于OneHotEncoding后的压缩吧", "我觉得是的"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>自编码器</p></div>", "lvl2_answer": ["这个应该只能用在神经网络上吧", "Autoencoder是神经网络，但是输出的降维结果也可以用在其他模型上"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><span style=\"font-size: 14px;\">Regularized Regression</span></div>", "lvl2_answer": ["$L_0$或者$L_1$的应该可以做到降维。$L_2$应该就不行了。"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>流形学习里有一些降维方法，比如PCA和Multidimensional scaling (MDS，多为尺度)。</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>线性判别分析:</p><p>PCA（主成分分析）和LDA（线性判别分析）有很多的相似点，其本质是要将初始样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。</p><p>推荐学习链接:https://www.zhihu.com/people/wang-he-13-93/posts</p><p><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>卷积算吗</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><ul><li><span style=\"font-size: 14px;\">PCA</span></li><li><span style=\"font-size: 14px;\">LDA</span></li><li><span style=\"font-size: 14px;\">SVD</span></li></ul></div>", "lvl2_answer": ["SVD不就是PCA降维？SVD是实现PCA的方法吧", "恩，之前的理解存在问题，谢谢提醒"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>autoencoder，其实pca也算是autoencoder的一种特殊情况吧</p></div>", "lvl2_answer": []}]}