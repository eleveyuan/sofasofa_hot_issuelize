{"id": "1001889", "question": "L2-norm为什么会让模型变得更加简单？", "description": "<div class=\"col-md-11 col-xs-10\"><p>众所周知，L1-norm通过稀疏规则将部分无用特征的权值置0，可将模型简化。而L2-norm仅仅是将每个特征的权值尽量地去减小，这样怎么就会让模型变得简单了呢？</p></div>", "viewer": 5155, "tags": ["统计/机器学习"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>我的理解：有规则的数据在特定的特征向量空间里，应该是“简单”（稀疏）的；相对的白噪音，因为没有规律，在任何特征向量空间都是“复杂”（dense）的。比如说固定频率的信号，在傅里叶变换中只有一个非零值，而白噪音的投影全是非零值。</p><p>最稀疏的prior应该是L0-norm（数个数），但因为是NP-hard问题，没法求解；然后就用L1-norm（减小绝对值和）去代替L0-norm；L2-norm的洗属性更弱一点，不能允许有大的参数，但是很难让参数为0。</p><p>各种Norm都可以让系数变得稀疏，只是强度不同，还有对系数值的分布也不同。比如L2-norm(x)对应的是x满足Guassian分布。L1-norm对应Laplace。<br/></p><p>举个例子：一共10个未知数x，Lp-norm（x）=100, p=0,1,2...。你可以看当p等于不同值时，满足以上条件的x的分布。</p></div>", "lvl2_answer": ["稀疏为什么对应着简单？", "稀疏也就是模型参数少了，也就简单了"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>这要看你怎么理解这个“简单”“复杂”了。</p><p>斯坦福的一个讲义里说，在特征都被标准化处理的情况下，回归系数越大，模型的复杂度越大。所以特征系数变成0，模型复杂度会下降；系数变小，当然也是复杂度下降。</p><p><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px;\">平滑也是简单的一种体现</span></p><p><span style=\"font-size: 14px;\"><br/></span></p></div>", "lvl2_answer": []}]}