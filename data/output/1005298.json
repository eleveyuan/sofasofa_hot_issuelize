{"id": "1005298", "question": "激活函数leakyRELU和pRELU的区别是什么？", "description": "<div class=\"col-md-11 col-xs-10\"><p>leakyRELU和pRELU的公式都是</p><p>如果 x &lt;   0：f(x) = ax</p><p>如果 x &gt;= 0：f(x) = x</p><p>感觉它们两个完全等价吧，那为什么又会有两个不同的名字呢</p></div>", "viewer": 7218, "tags": ["统计/机器学习", "人工神经网络"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px;\">并不完全等价，leakyrelu的a是固定的， prelu可以看作是leakyrelu的一个变体，a是根据网络训练得到的，而非预先定义的。</span></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>两个激活函数都是对RELU的进一步推广。</p><p>对于LeakyRELU，$f(x)=\\alpha x$中的是固定的，是模型训练前提前设定的。</p><p>对于pRELU，$f(x)=\\alpha x$中的是未知的，是模型训练中得到的。</p><p>更简洁一点说，LeakyRELU中$\\alpha$是超参，pRELU中$\\alpha$是参数。（两者的区别看这里<a href=\"http://sofasofa.io/forum_main_post.php?postid=1001165\" target=\"_blank\">hyperparameter与parameter</a>）</p></div>", "lvl2_answer": []}]}