{"id": "1655938", "question": "做多元线性回归的时候，怎么快速排除掉没有用的特征？", "description": "<div class=\"col-md-11 col-xs-10\"><p>做多元线性回归的时候，特征很多，有的甚至对模型精度提升是副作用，怎么快速筛选出这些特征，并且排除掉这些没有用的特征？</p></div>", "viewer": 92, "tags": ["统计/机器学习", "回归分析", "特征选择"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>快速排除的话，有几个方法：</p><p>1）可以考虑单因子模型，单因子预测效果最差的特征可以排除。</p><p>2）看特征的缺失值占比，缺失值占比过高的，可以直接剔除</p><p>3）看特征方差，特征是常数或者接近为常数的，可以直接剔除</p><p>4）做LASSO，被LASSO剔除的变量可以不考虑</p><p>5）重复的特征，比如两个特征几乎一样，也可以直接剔除<br/></p><p><br/></p><p>如果复杂一点、精确一点的话，可以做逐步特征选择（向前或者向后），AIC、BIC，<a href=\"http://sofasofa.io/forum_main_post.php?postid=1001641\" target=\"_blank\">feature importance</a>，<a href=\"http://sofasofa.io/forum_main_post.php?postid=1000484\" target=\"_blank\">VIF方法</a>等等，做交叉验证来看哪些特征需要剔除掉。<br/></p></div>", "lvl2_answer": []}]}