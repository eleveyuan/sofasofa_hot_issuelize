{"id": "1003081", "question": "如何计算加权最小二乘法的样本权重？", "description": "<div class=\"col-md-11 col-xs-10\"><p>如果想用加权最小二乘法进行回归，如何计算每个样本该加多少的权重呢？</p></div>", "viewer": 8762, "tags": ["统计/机器学习", "回归分析", "损失函数"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>最小二乘是求残差$e=Xw-y$的最大似然，也就是求最小-log似然。为简化问题，假设$e$是样本独立的nx1高斯分布,n是数据个数，$e$的nxn协方差矩阵$\\Sigma_{ee}$是对角线矩阵，每个数据点在loss function中的权重是其残差的方差的倒数$1/var(e_i)$。换句话说，$e_i$方差越大，越不可信，其权重越小。而$e_i$方差的估计由实际问题的统计模型决定，我觉得是最小二乘框架中最重要，最体现对实际问题理解程度的地方。</p><p>$$\\Sigma_{ee}=\\begin{bmatrix} var(e_1) &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; var(e_n) \\end{bmatrix}$$</p><p>$$\\Sigma_{ee}^{-1}=\\begin{bmatrix} 1/var(e_1) &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; 1/var(e_n) \\end{bmatrix}$$</p><p>数据点加权的-logloss function是<br/></p><p>$$-\\log L(w)=(Xw-y)^T\\Sigma_{ee}^{-1}(Xw-y)$$</p><p>因为$\\Sigma_{ee}^{-1}$是对角线矩阵，有</p><p>$$-\\log L(w)=\\Sigma_{ee}^{-1}(Xw-y)^T(Xw-y)$$</p><p>其中$w$是待求参数，$X$是输入数据，$y$是输出，$\\Sigma_{ee}$是输出的covariance matrix。每个数据对应的-logloss的权重是$1/var(e_i)$。</p><p><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>没有权重的线性回归</p><p>$$y_i=X_i\\beta + \\beta_0+\\sigma_i$$</p><p>$\\sigma_i$是拟合的残差。根据线性回归的基本假设，残差是相等的，所以为了确保这一点，我们给每个样本增加权重</p><p>$$w_i=\\frac{1}{\\sigma_i^2}$$</p></div>", "lvl2_answer": []}]}