{"id": "1001122", "question": "神经网络中的gradient check是什么意思？", "description": "<div class=\"col-md-11 col-xs-10\"><p>神经网络中的gradient check是什么意思？就是检查梯度吗？为什么要查梯度？</p><p><br/></p><p><br/></p></div>", "viewer": 4410, "tags": ["统计/机器学习", "深度学习", "人工神经网络"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>人工神经网络是个非常复杂的结构，一层层叠加而成。</p><p>复杂就意味着在实施人工神经网络的时候需要格外小心，其中一个步骤就是手动检查反向传播（backpropagation）是否正确运行。</p><p>所以我们就要手动计算和比较真实的梯度和程序中返回的梯度值，以确保其正确。</p><p><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>主要是解决<a href=\"https://en.wikipedia.org/wiki/Vanishing_gradient_problem\" target=\"_blank\">vanishing gradient problem</a>。大概意思是backpropogation通过链式规则把gradient从输出端的loss function要反向传输到第一层的参数。传输过程中，gradient会逐渐减小或增大，会导致第一层的参数收到的gradient过小或过大。还有个主要原因是所有参数用同样的learning rate。如果用动态的learning rate，会一定程度解决问题。</p><p>解决的方法有：</p><p>1. Resnet，加一些bypass的路径，让gradient能通过较少层达到前几层的参数。</p><p>2. Relu比sigmoid传输gradient更快。记忆中，AlexNet论文中说Relu收敛比sigmoid快3倍。</p><p>3. 更好的参数初始值。让链式规则中的gradient乘上系数的绝对值集中在1，也就是Jacobian 矩阵元素的绝对值大概为1。参考<a href=\"https://arxiv.org/abs/1502.01852\" target=\"_blank\">He Kaiming的论文</a>。</p><p>4. 在中间层增加一些辅助的loss function，让辅助的gradient能提前传到前面的层。有些论文叫hierarchical learning。</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>可以参考下stanford这篇文章<a href=\"http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization\">Gradient checking and advanced optimization</a></p><p><br/></p></div>", "lvl2_answer": []}]}