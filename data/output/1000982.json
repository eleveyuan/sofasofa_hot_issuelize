{"id": "1000982", "question": "在使用PCA降维时，有哪些坑？", "description": "<div class=\"col-md-11 col-xs-10\"><p>我有一个分类问题，列数很多，我试了下PCA降维，然后再训练分类器，可是结果反而差了不少。</p><p>不知道有没有人遇到过类似的情况，或者是我不小心踩进了PCA的坑？</p><p>大神们可以分享下PCA的坑吗？或者成功使用PCA的经验吗？</p><p><br/></p></div>", "viewer": 10545, "tags": ["统计/机器学习", "监督式学习", "无监督学习", "数据降维", "开放问题"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px;\">除了各位说的，我再补充一些我所认为的坑</span></p><p><span style=\"font-size: 14px;\"><br/></span></p><ul><li><b><span style=\"font-size: 14px;\">PCA依赖于线性假设。</span></b></li></ul><p><span style=\"font-size: 14px;\">            我们知道</span><a href=\"http://sofasofa.io/forum_main_post.php?postid=1000884\" style=\"font-size: 1rem; background-color: rgb(255, 255, 255);\" target=\"_blank\"><span style=\"font-size: 14px;\">PCA本质上就是SVD</span></a><span style=\"font-size: 14px;\">，所以PCA不过是利用线性投影进行降维。但是数据的内在结构未必是简单的线性关系。</span></p><p><span style=\"font-size: 14px;\"><br/></span></p><ul><li><b><span style=\"font-size: 14px;\">PCA是正交的。</span></b></li></ul><p><span style=\"font-size: 14px;\">            正如第一点所说，PCA是正交的投影，可是未必正交的就是最好的。想象一下，姚明和郭敬明（两个三维物体），我们用灯去照他们的影子，当灯光从两人正上方10米向下照的时候，两个人影子都是一坨圆；如果我们从侧面照他们，投影出来的两个影子，可能宽度差不多，长度却差不少，这样的影子（三维降到二维）就含有更多信息。</span></p><p><span style=\"font-size: 14px;\"><br/></span></p><ul><li><b><span style=\"font-size: 14px;\">PCA让方差尽量大。</span></b></li></ul><p><span style=\"font-size: 14px;\">            PCA本质上就是抓住主体，放过细枝末节。但有时候细枝末节往往是很重要的。五个韩国姑娘，其中一个姑娘鼻子上有痣，于是你一眼就认出了。你对她们的照片做PCA，之后发现，大家的脸型五官都没变，但是那个痣却找不到了，那你还能认出那个姑娘吗？你的算法还能认出那个姑娘吗？</span></p><p><span style=\"font-size: 14px;\"><br/></span></p><ul><li><span style=\"font-size: 14px;\"><b>PCA的可解释性</b></span></li></ul><p><span style=\"font-size: 14px;\">             如果PCA之后需要使用线性模型，那么这个线性模型大概率也是不可解释的。因为PCA本身是特征重组压缩，原本的变量的意义会消失。</span><br/></p></div>", "lvl2_answer": ["PCA是个差强人意的照妖镜"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>一定要记得先做标准化。参考<a href=\"http://sofasofa.io/forum_main_post.php?postid=1000375\">这个</a>。</p><p><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>我也说个我的。我之前是把training set的特征和test set的特征和一起做了PCA降维。</p><p>其实这是大错特错的，因为在PCA中用test set的特征，这属于信息泄露。</p><p>正确的方法是在training set上fit_transform，在test set上transform。</p><p><br/></p><p><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>降维不能降太多，不然丢失了太多信息，反而不利于预测</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>PCA降为前需要把相关性高的那些列去掉，不然会影响到PCA的效果。</p><p><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>说明你的样本量不够大。</p><p>为什么你要PCA降维？ 因为你的数据样本大到上千万行几十列，必须用hadoop进行算法计算，而你只有一台1万的台式机，你需要做一定的数据信息舍弃，放弃精准度，得到一个95%或者90%近似正确的结果。</p><p>PCA降维的核心理念是，抓住核心部分，放弃一些有影响力但是却不那么重要的列，以此减少算法的计算复杂度。</p><p>如果你PCA降维后反而效果更差了，说明你的样本量（行数）不够大，放弃的那些列对结果有“较大”的影响。</p><p>所以，别人扛着<span style=\"color: rgb(255, 0, 0);\">三百斤</span>的大石头（大数据），为了方便，造了一辆马车（PCA降维）。不代表，你扛着<span style=\"color: rgb(255, 0, 0);\">三斤</span>的石头（你的数据集），也需要一辆马车（PCA降维）。在现实的情况中，如果BAT是三百斤的大石头，可能你扛的只是0.00003斤的石头，放口袋里就可以了，不需要马车。</p><p>每个数据集都是不一样的烟火，抛开了数据集属性谈算法，都是耍流氓。</p></div>", "lvl2_answer": []}]}