{"id": "1007419", "question": "为什么GBDT比RF更容易overfitting？", "description": "<div class=\"col-md-11 col-xs-10\"><p>GBDT是不是比RF更容易overfitting？如果是的话，为什么？</p></div>", "viewer": 2972, "tags": ["统计/机器学习", "监督式学习"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>GBDT是串行的，损失函数定下后集成树的生成全部以极小化损失函数为目标，后面的树一直在弥补前面的树犯下的错误，和并行结构的RF相比更容易过拟合。所以GBDT会采用更简单更浅的决策树防止过拟合，学习率与正则化项的引入也是为了这一目的，防止学得太快太猛，增加不同的声音。一点浅见，如有错误还请指正。</p></div>", "lvl2_answer": ["谢谢"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>随机森林和分类树相比是减少了方差，GBDT和随即森林比是减少了偏差。偏差约减，越容易过拟合。</p><p>GBDT在训练的过程中，随着树的个数的增加，训练偏差会一直减小，甚至减到0，这时候测试误差反而会很大，这个就是过拟合了。对于随机森林，训练误差不会随着树的增加而一直减少，所以不是特别容易过拟合。</p></div>", "lvl2_answer": ["嗯，是的，所以gbdt有早停这个概念吧"]}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><b>Random Forest</b><br/><br/>​采用bagging思想，即利用bootstrap抽样，得到若干个数据集，每个数据集都训练一颗树。<br/><br/>构建决策树时，每次分类节点时，并不是考虑全部特征，而是从特征候选集中选取若干个特征用于计算。弱特征共有p个，一般选取m=sqrt(p)个特征。当可选特征数目很大时，选取一个较小的m值，有助于决策树的构建。<br/><br/>​当树的数量足够多时，RF不会产生过拟合，提高树的数量能够使得错误率降低。<br/><br/><b>GBDT</b><br/><br/>采用Boosting思想（注意是Boosting，不是Boostrap）​<br/><br/>不采用Boostrap抽样的方法（RF采用了），每次迭代过程都会使用全部数据集（会有一些变化，即采用的是上一轮训练后得到的预测结果与真实结果之间的残差（残差是由损失函数计算得到的））。<br/><br/>​GBDT的每棵树是按顺序生成的，每棵树生成时都需要利用之前一棵树留下的信息（RF的树是并行生成的）。<br/><br/>​GBDT中树的数目过多会引起过拟合（RF不会）。<br/><br/>​构架树时，深度为1时通常效果很好（深度为1的决策树称为决策桩decision stumps）。<br/></p></div>", "lvl2_answer": []}]}