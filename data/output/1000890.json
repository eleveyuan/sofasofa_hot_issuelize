{"id": "1000890", "question": "PCA降维中的特征值和特征向量", "description": "<div class=\"col-md-11 col-xs-10\"><p>特征值和特征向量在数学上如何推导出来的我大概知道，但是对这两个概念有没有更直观的解释？</p></div>", "viewer": 7203, "tags": ["统计/机器学习", "无监督学习", "数据降维", "特征选择"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>PCA中的特征值其实就是对应的SVD的奇异值的平方。</p><p>PCA主要是对协方差矩阵$XX^T$进行特征分解</p><p>$$XX^T=U\\Sigma U^T$$</p><p>$\\Sigma$是个对角阵，对角线上的元素就是特征值。$U$的列向量就是特征向量。</p><p><br/></p><p>也可以参考我在另外一个问题里写的答案<a href=\"http://sofasofa.io/forum_main_post.php?postid=1000884\">PCA和SVD是一回事吗？</a></p><p><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>一个矩阵点乘一个向量时，可以把矩阵看成是一个线性变换，点乘就是把这个线性变换施加到相应的向量上。当线性变换施加到该矩阵的特征向量上时，并不改变特征向量的方向，只是对其进行拉长或者缩短。而这个拉长或者缩短的程度就是由相应的特征值所决定。</p><p>如果特征值为2，那就是把该特征向量的长度变为原来的两倍。</p><p>如果特征值为-2，那就是把向量方向改变180度，并把长度变为原来两倍。</p></div>", "lvl2_answer": []}]}