{"id": "1002656", "question": "用SGD计算逻辑回归时的迭代公式是什么？", "description": "<div class=\"col-md-11 col-xs-10\"><p>SGD的公式如下，alpha后面的是在xi, yi点的损失函数的梯度</p><p><img data-filename=\"捕获.JPG\" src=\"data:image/jpeg;base64,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\" style=\"width: 363px;\"/><br/></p><p>但是具体对于逻辑回归来说，迭代公式是什么？也就是说逻辑回归损失函数的梯度应该是什么？</p></div>", "viewer": 5012, "tags": ["统计/机器学习", "回归分析", "监督式学习", "损失函数"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>$\\triangledown _{\\theta }J( \\theta;x_{i},y_{i})$</p><p>=$-\\frac{\\partial }{\\partial \\theta}(y_{i}log(h_{ \\theta}(x_{i}))+(1-y_{i})log(1-h_{ \\theta}(x_{i})))$ <b>（交叉熵损失）</b></p><p>=$-\\frac{y_i}{h_{ \\theta}(x_i)}\\frac{\\partial h_{ \\theta}(x_i)}{\\partial \\theta}+\\frac{1-y_i}{1-h_{ \\theta}(x_i)}\\frac{\\partial h_{\\theta}(x_i))}{\\partial \\theta}$ </p><p>=$-\\frac{y_i}{h_{\\theta}(x_i)}h_{\\theta}(x_i)(1-h_{\\theta}(x_i)))x_i+\\frac{1-y_i}{1-h_{\\theta}(x_i)}h_{\\theta}(x_i)(1-h_{\\theta}(x_i))x_i$<b>（sigmoid的导数公式）</b></p><p>=$-y_i(1-h_{\\theta}(x_i))x_i)+(1-y_i)h_{\\theta}(x_i)x_i$</p><p>=$-y_ix_i+y_ih_{\\theta}(x_i)x_i+h_{\\theta}(x_i)x_i1y_ih_{\\theta}(x_i)x_i$</p><p>=$(h_{\\theta}(x_i)-y)x_i$</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>第$j$个coefficient update的公式</p><p>$$\\theta_j:=\\theta_j - \\alpha (h_\\theta(x^r) - y^r)x^r_j$$</p><p>$h_\\theta(x^r)$是当前模型对某个随机样本$x^r$的预测值，$x^r_j$是第$r$个样本中第$j$个变量</p><p><br/></p><p>写成向量的形式就是</p><p>$$\\theta:=\\theta - \\alpha (h_\\theta(x^r) - y^r)x^r$$</p></div>", "lvl2_answer": []}]}