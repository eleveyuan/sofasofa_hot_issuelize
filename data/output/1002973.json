{"id": "1002973", "question": "Lasso是对数值大的系数压缩大还是数值小的系数压缩大？", "description": "<div class=\"col-md-11 col-xs-10\"><p>比如我有两个变量$X_1$, $X_2$，做完线性回归之后发现它们的系数分别为</p><p>$$\\beta_1=10, \\beta_2=1$$</p><p>现在我对这个线性回归加$L_1$的惩罚项，也就是说模型变成了LASSO。</p><p>按照LASSO的原理，$\\beta_1$和$\\beta_2$应该变小。那么谁变小的幅度更大呢？还是说它们是等比例的变小？</p></div>", "viewer": 3876, "tags": ["统计/机器学习", "回归分析", "监督式学习", "特征选择"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>LASSO的loss有两项分别是likelihood和prior。Loss对于$\\beta$的gradient也分成两项。来至于likelihood的gradient和问题本身相关，很难比较$grad_{\\beta_1}$和$grad_{\\beta_2}$的大小。我们只能比较来至于prior的gradient。</p><p>$d(|\\beta|)/d\\beta=sign(\\beta)$</p><p>也就是说来至于prior的改变量的绝对值是一个常数$C$（$C=\\alpha*\\lambda$,$\\alpha$是learning rate，$\\lambda$是拉格朗日乘数，用于平衡likelihood和prior）。假如$C=0.1，\\beta_1=10,\\beta_2=1$，那么$\\beta_1$和$\\beta_2$只通过prior能变成0的最少步数分别是100步和10步。</p><p>如果是RIDGE，来至于prior的gradient</p><p>$d(|\\beta|^2)/d\\beta=2\\beta$</p><p>随着$\\beta$的减小，改变的速度也会下降。</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px; line-height: 1.5;\">Ridge回归是等比例衰减</span></p><p><span style=\"font-size: 1rem; line-height: 1.5;\"><span style=\"font-size: 14px;\">lasso的话要分情况，有一种情况是还直接变为0.</span><br/></span><br/></p></div>", "lvl2_answer": []}]}