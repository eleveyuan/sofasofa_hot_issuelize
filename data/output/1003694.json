{"id": "1003694", "question": "为什么GAN是非监督的？", "description": "<div class=\"col-md-11 col-xs-10\"><p>为什么GAN是非监督的学习方法？</p></div>", "viewer": 6734, "tags": ["统计/机器学习", "无监督学习", "深度学习", "人工神经网络"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>因为GAN本身并不需要数据有label，GAN是产生人工“假”数据，配合原来的真数据来训练discriminator ，而并非原始数据中的label。</p><p>所以说GAN是非监督的。</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px;\">GAN是semi-supervised, 包含generator$G()$和discriminator$D()$。它有两个输出数据，对应两个loss：</span></p><p><span style=\"font-size: 14px;\">1.原始的输出。loss1可以是逻辑回归或线性回归，用于训练$G()$，是监督训练；</span></p><p><span style=\"font-size: 14px;\">2.人造的label，表示是见过的训练数据。loss2是逻辑回归，用于训练 $D(G())$，是非监督训练。关键是loss2的gradient会传到$G()$，从而用人造label训练generator。换句话说人造label会作为正则项限制$G()$中参数的取值范围。Discriminator用到了“存在即合理”的假设，label=1表示“存在”。这点类似于one class svm anomaly detection，把见过的正常数据的表达空间压缩到一个点上（label=1），而label=0表示其余没见过的“非正常”数据。</span></p><p><span style=\"font-size: 14px;\">----------------题外话---------------------</span></p><p><span style=\"font-size: 14px;\">Discriminator可看做是一个正则项，类似对generator参数$w$的L1/L2 norm，提供先验信息（prior），压缩$w$的取值空间$W$。一般overfitting都发生在$W$的边沿，如果限缩了$W$，把那些边沿部分从$W$中去掉，会降低overfitting的概率。</span></p><p><span style=\"font-size: 14px;\">正则项一般来源于数据某种特征的稀疏性，比如$w$的L1/L2 norm，或者图像处理中相邻pixel的差要稀疏（TV, Laplace，Gaussian）。 </span>Discriminator也是一个稀疏表达（sparse representation）的限制条件，这个人造label就是它的特征，把所有训练数据压缩到一个点上。</p><p><br/></p></div>", "lvl2_answer": []}]}