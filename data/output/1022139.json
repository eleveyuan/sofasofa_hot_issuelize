{"id": "1022139", "question": "线性支持向量机和中心化中截距的问题", "description": "<div class=\"col-md-11 col-xs-10\"><p>我又两个问题：1、在线性支持向量机中，优化函数是正则项和损失项的和，其中为什么正则项不包含截距项呢。2、对每个特征做中心化处理之后可以将截距项直接去除，这是为什么呢。求大佬解答~</p></div>", "viewer": 1718, "tags": ["统计/机器学习", "回归分析"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>第一个问题可以参考一下<a href=\"http://sofasofa.io/forum_main_post.php?postid=1002766\" target=\"_blank\">Lasso和岭回归的正则项包含截距（常数项）吗？</a></p><p>第二个问题：所有的最小二乘回归方程都会经过$(\\bar x, \\bar y)$这一个点，如果你对所有的自变量以及y进行中心化，也就是说自变量的均值$\\bar x=0$，因变量的均值$\\bar y=0$。一个线（或者超平面）要经过$(0,0)$这个点，那么截距一定是0。</p></div>", "lvl2_answer": ["谢谢！第二个问题可能有点问题吧，最小二乘法只是在线性回归中，存在解析解，中心化之后是去掉截距的。但是线性支持向量机优化不是用的梯度下降法吗，损失函数为合页损失函数，这个解和中心化有关系吗？", "@matt 回答的是最小二乘，你问的svm，应该是他看错了"]}]}