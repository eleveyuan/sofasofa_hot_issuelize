{"id": "1000740", "question": "线性回归的bagging", "description": "<div class=\"col-md-11 col-xs-10\"><p>我们经常看到Tree bagging，因为它是决策树的改良。</p><p>那我可以对线性回归bagging吗？降低它的预测方差，提高稳定性？</p></div>", "viewer": 5808, "tags": ["统计/机器学习", "回归分析", "监督式学习"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>你<span style=\"font-weight: bold;\">可以</span>对线性回归模型进行Bagging，但是<span style=\"font-weight: bold;\">毫无意义</span>。</p><p><br/></p><p>决策树需要做bagging，是因为它的预测bias小，variance大，所以可以通过bagging降低variance。</p><p><br/></p><p>线性回归则是完全相反，bias大，variance非常小。从这个角度看，bagging并不能够改善什么。最重要的是，<span style=\"font-weight: bold;\">bagging对线性回归并不起作用</span>。因为bagging会对多个单个模型的预测结果加和求平均，多个线性方程的平均，依然是个线性方程。所以当对很多线性回归模型求平均之后，最终得到模型的变量系数的还是你最初的线性方程的系数。</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>线性回归的bagging是没有意义的。</p><p>但是广义线性模型，比如逻辑回归，bagging却是有意义的。我们不是bagging logit(p)，而是bagging最终的p。</p><p><br/></p></div>", "lvl2_answer": ["是的，对GLM做bagging其实效果还不错"]}]}