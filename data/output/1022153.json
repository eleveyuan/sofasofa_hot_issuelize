{"id": "1022153", "question": "神经网络里为什么隐藏层越多越好？", "description": "<div class=\"col-md-11 col-xs-10\"><p>神经网络里为什么隐藏层越多越好？</p></div>", "viewer": 4318, "tags": ["统计/机器学习", "深度学习", "人工神经网络"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px;\">神经网络是非线性函数聚类+线性拟合。非线性函数作用是把每层输入特征空间分割成很多小的子空间，每个子空间是一个簇；然后每个神经元在一个子空间做线性回归。类似于用很短的线段去描任意曲线。</span></p><p><span style=\"font-size: 14px;\">隐藏层越多，特征空间的抽象程度越高，分割的子空间也会更小，在子空间内的数据点可能会更线性相关。打个比方，以体重划分动物，找不出人类；如果以是否用手制造工具并有复杂语言为特征，更容易把人类找出来。</span></p><p><span style=\"font-size: 14px;\">隐藏层越多，训练越困难，梯度消失会更严重。还有个问题是隐藏层越多，特征抽象程度会越高，更容易过拟合。</span><span style=\"font-size: 14px; -webkit-tap-highlight-color: transparent; text-size-adjust: 100%;\">需要更大训练数据解决过拟合。</span></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>神经网络是利用中间的隐藏层去学习原来在机器学习中的特征，如果隐藏层越多，那么神经网络学习到的特征就越多，那么自然学习的效果越好。但是神经网络的隐藏层不是越多越好，因为存在梯度爆炸与梯度消失问题，神经网络的层数是有存在一定限制的，同时神经网络的隐藏层越多，那么相应需要学习的参数就越多，训练时间与训练难度就越大，所以隐藏层不是越多越好。</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>隐层越多，体现模型的 学习能力更强，能学到更多东西。但是我一般是根据实际数据训练的反馈情况来做调整的，不一定越多越好。我目前是这样理解的。</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>越后面的隐藏层学得越精细，也比较抽象难解释。隐藏越多，参数越多，对数据量的要求也更大。</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>肯定不是越多越好的，物极必反。需要看任务本身的复杂度和数据本身的特性。</p><p>“隐藏层越多越好”这个观点也违背奥卡姆剃刀原则了。</p></div>", "lvl2_answer": []}]}