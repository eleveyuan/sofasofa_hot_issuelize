{"id": "1002735", "question": "为什么基于skip-gram的word2vec在低频词汇相比cbow更有效？", "description": "<div class=\"col-md-11 col-xs-10\"><p>面试题库里卷38里的一道题目</p><p>对于基于skip-gram和基于CBOW的word2vec，哪个模型对低频词汇表现更好？</p><p>答案是skip-gram</p><p>不是非常理解，求大神分析下</p></div>", "viewer": 6225, "tags": ["统计/机器学习", "自然语言处理", "人工神经网络"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p><span style=\"font-size: 14px;\">CBOW是根据上下文预测当中的一个词，也就是用多个词预测一个词</span></p><p><span style=\"font-size: 14px;\">比如这样一个句子</span><span style=\"font-size: 14px; background-color: rgb(239, 239, 239);\">yesterday was really a [...] day</span><span style=\"font-size: 1rem;\"><span style=\"font-size: 14px;\">，中间可能是</span><span style=\"background-color: rgb(239, 239, 239); font-size: 14px;\">good</span><span style=\"font-size: 14px;\">也可能是</span><span style=\"background-color: rgb(239, 239, 239); font-size: 14px;\">nice</span><span style=\"font-size: 14px;\">，比较生僻的词是</span><span style=\"background-color: rgb(239, 239, 239); font-size: 14px;\">delightful</span><span style=\"font-size: 14px;\">。当CBOW去预测中间的词的时候，它只会考虑模型最有可能出现的结果，比如</span><span style=\"background-color: rgb(239, 239, 239); font-size: 14px;\">good</span><span style=\"font-size: 14px;\">和</span><span style=\"background-color: rgb(239, 239, 239); font-size: 14px;\">nice</span><span style=\"font-size: 14px;\">，生僻词</span><span style=\"background-color: rgb(239, 239, 239); font-size: 14px;\">delightful</span><span style=\"font-size: 14px;\">就被忽略了。</span></span></p><p><span style=\"font-size: 14px;\">而对于</span><span style=\"background-color: rgb(239, 239, 239); font-size: 14px;\">[...] was really a delightful day</span><span style=\"font-size: 14px;\">这样的句子，每个词在进入模型后，都相当于进行了均值处理（权值乘以节点），</span><span style=\"font-size: 14px; background-color: rgb(239, 239, 239);\">delightful</span><span style=\"font-size: 14px;\">本身因为生僻，出现得少，所以在进行加权平均后，也容易被忽视。</span></p><p><span style=\"font-size: 14px;\">Skip-Gram是根据一个词预测它的上下文，也就是用一个词预测多个词，每个词都会被单独得训练，较少受其他高频的干扰。所以对于生僻词Skip-Gram的word2vec更占优。</span></p><p><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>在 Google Groups 中，Milokov 提到：</p><p>“Skip-gram: works well with small amount of the training data, represents well even rare words or phrases\n</p><p>CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words\n</p><p>This can get even a bit more complicated if you consider that there are two different ways how to train the models: the normalized hierarchical softmax, and the un-normalized negative sampling. Both work quite differently.”</p><p><br/></p><p>关于这段话，stackoverflow 就数据量这个问题进行过讨论：<a href=\"https://stackoverflow.com/questions/39224236/word2vec-cbow-skip-gram-performance-wrt-training-dataset-size\" target=\"_blank\">https://stackoverflow.com/questions/39224236/word2vec-cbow-skip-gram-performance-wrt-training-dataset-size</a></p><p><br/></p><p>但关于低频词汇的有效性，并没有过多的说明，我是这样反向理解的：由于 CBOW 需要更多的数据，所以它对高频词汇更敏感，从而在低频词汇上表现没有 skip-gram 好。</p><p><br/></p><p>觉得这样解释不是很好，欢迎补充！</p></div>", "lvl2_answer": []}]}