{"id": "1000762", "question": "随机梯度下降(sgd)的收敛问题", "description": "<div class=\"col-md-11 col-xs-10\"><p>对于凸优化来说，剃度下降（批量剃度下降）是肯定会收敛到全局最优的。那么对于凸问题，sgd也会收敛到全局最优吗？我在线性回归上试了一下，发现即使很多次迭代之后，回归系数一直在波动，看起来并没收敛。</p></div>", "viewer": 11867, "tags": ["数学", "数值计算", "最优化"], "answers": [{"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>随机剃度下降是不会收敛的，它总是在最优点附近跳来跳去。即使我们到达了最优点，它依然会跳动，因为对于随机的样本来说，这些少数的样本在最优点的梯度也未必是0（整体的梯度是0）。</p><p><br/></p><p>有一个方法可以使随机梯度下降收敛——让步长衰减。因为随机梯度很快就能到达最优点附近，如果步长逐步减小，最终它会停在最优点附近一个较优的点上（未必是正好停在最优点）。</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>是的，不收敛的。随机梯度下降（Stochastic Gradient Descent）和小批量梯度下降（mini-batch）都存在这个现象。</p><p>现在很多的算法和一些封装好的package在使用SGD或者类似算法的时候都使用了不固定的learning rate（步长），其目的就是为了解决不收敛。</p><p><br/></p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>凸优化的全局最优点是针对训练数据而言的，更换了当前训练数据，当前的最优点就变了。所以SGD本来就没有固定的全局最优点。最后得到的是多个batch上最优点的一个或几何均值。比如多个batch上最优点组成一个圆，那么最后结果就是圆内随机一点。因为GD用了全部训练数据，所以最优点固定，是圆的重心。</p><p>以简单二维输入最小二乘为例，每一个训练数据产生loss funtion的曲面是以此点对应系数为中心的倒钟形，有点像沙坑里落下一个铅球，每个点都产生一个坑。一个batch的数据生成的坑的乘积就是batch对应的loss funtion的曲面。换一组铅球的话，最低点自然就会移动。</p></div>", "lvl2_answer": []}, {"lvl1_answer": "<div class=\"col-md-11 col-xs-10 p-r\"><p>随机类型的算法，看收敛性质是看Expectation期望的，是error的期望值收敛。</p></div>", "lvl2_answer": []}]}